{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 4980,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004016064257028112,
      "grad_norm": 1.3410128355026245,
      "learning_rate": 0.00019983935742971887,
      "loss": 2.6242,
      "step": 10
    },
    {
      "epoch": 0.008032128514056224,
      "grad_norm": 0.17546769976615906,
      "learning_rate": 0.00019957161981258367,
      "loss": 0.1865,
      "step": 20
    },
    {
      "epoch": 0.012048192771084338,
      "grad_norm": 0.2884160578250885,
      "learning_rate": 0.00019930388219544846,
      "loss": 0.1674,
      "step": 30
    },
    {
      "epoch": 0.01606425702811245,
      "grad_norm": 0.16600263118743896,
      "learning_rate": 0.00019903614457831325,
      "loss": 0.1931,
      "step": 40
    },
    {
      "epoch": 0.020080321285140562,
      "grad_norm": 0.22165589034557343,
      "learning_rate": 0.00019876840696117805,
      "loss": 0.2165,
      "step": 50
    },
    {
      "epoch": 0.024096385542168676,
      "grad_norm": 0.1574738323688507,
      "learning_rate": 0.00019850066934404287,
      "loss": 0.1706,
      "step": 60
    },
    {
      "epoch": 0.028112449799196786,
      "grad_norm": 0.17505013942718506,
      "learning_rate": 0.00019823293172690763,
      "loss": 0.1834,
      "step": 70
    },
    {
      "epoch": 0.0321285140562249,
      "grad_norm": 0.17651118338108063,
      "learning_rate": 0.00019796519410977242,
      "loss": 0.1879,
      "step": 80
    },
    {
      "epoch": 0.03614457831325301,
      "grad_norm": 0.14281855523586273,
      "learning_rate": 0.00019769745649263722,
      "loss": 0.1622,
      "step": 90
    },
    {
      "epoch": 0.040160642570281124,
      "grad_norm": 0.1657632738351822,
      "learning_rate": 0.000197429718875502,
      "loss": 0.1685,
      "step": 100
    },
    {
      "epoch": 0.04417670682730924,
      "grad_norm": 0.16495706140995026,
      "learning_rate": 0.0001971619812583668,
      "loss": 0.1576,
      "step": 110
    },
    {
      "epoch": 0.04819277108433735,
      "grad_norm": 0.2072693407535553,
      "learning_rate": 0.00019689424364123162,
      "loss": 0.2029,
      "step": 120
    },
    {
      "epoch": 0.05220883534136546,
      "grad_norm": 0.18284757435321808,
      "learning_rate": 0.00019662650602409642,
      "loss": 0.1569,
      "step": 130
    },
    {
      "epoch": 0.05622489959839357,
      "grad_norm": 0.14073553681373596,
      "learning_rate": 0.00019635876840696118,
      "loss": 0.1549,
      "step": 140
    },
    {
      "epoch": 0.060240963855421686,
      "grad_norm": 0.12602190673351288,
      "learning_rate": 0.00019609103078982597,
      "loss": 0.1727,
      "step": 150
    },
    {
      "epoch": 0.0642570281124498,
      "grad_norm": 0.1342284232378006,
      "learning_rate": 0.00019582329317269077,
      "loss": 0.1565,
      "step": 160
    },
    {
      "epoch": 0.06827309236947791,
      "grad_norm": 0.22000163793563843,
      "learning_rate": 0.00019555555555555556,
      "loss": 0.146,
      "step": 170
    },
    {
      "epoch": 0.07228915662650602,
      "grad_norm": 0.21481861174106598,
      "learning_rate": 0.00019528781793842035,
      "loss": 0.1875,
      "step": 180
    },
    {
      "epoch": 0.07630522088353414,
      "grad_norm": 0.1573336273431778,
      "learning_rate": 0.00019502008032128517,
      "loss": 0.1953,
      "step": 190
    },
    {
      "epoch": 0.08032128514056225,
      "grad_norm": 0.18404266238212585,
      "learning_rate": 0.00019475234270414994,
      "loss": 0.1469,
      "step": 200
    },
    {
      "epoch": 0.08433734939759036,
      "grad_norm": 0.2838520109653473,
      "learning_rate": 0.00019448460508701473,
      "loss": 0.18,
      "step": 210
    },
    {
      "epoch": 0.08835341365461848,
      "grad_norm": 0.14787264168262482,
      "learning_rate": 0.00019421686746987952,
      "loss": 0.2288,
      "step": 220
    },
    {
      "epoch": 0.09236947791164658,
      "grad_norm": 0.2074081003665924,
      "learning_rate": 0.00019394912985274432,
      "loss": 0.1606,
      "step": 230
    },
    {
      "epoch": 0.0963855421686747,
      "grad_norm": 0.19937831163406372,
      "learning_rate": 0.0001936813922356091,
      "loss": 0.2306,
      "step": 240
    },
    {
      "epoch": 0.10040160642570281,
      "grad_norm": 0.16007427871227264,
      "learning_rate": 0.0001934136546184739,
      "loss": 0.1774,
      "step": 250
    },
    {
      "epoch": 0.10441767068273092,
      "grad_norm": 0.16488371789455414,
      "learning_rate": 0.0001931459170013387,
      "loss": 0.1645,
      "step": 260
    },
    {
      "epoch": 0.10843373493975904,
      "grad_norm": 0.14890284836292267,
      "learning_rate": 0.0001928781793842035,
      "loss": 0.1331,
      "step": 270
    },
    {
      "epoch": 0.11244979919678715,
      "grad_norm": 0.21429581940174103,
      "learning_rate": 0.00019261044176706828,
      "loss": 0.1817,
      "step": 280
    },
    {
      "epoch": 0.11646586345381527,
      "grad_norm": 0.1565224975347519,
      "learning_rate": 0.00019234270414993307,
      "loss": 0.1747,
      "step": 290
    },
    {
      "epoch": 0.12048192771084337,
      "grad_norm": 0.15008574724197388,
      "learning_rate": 0.00019207496653279787,
      "loss": 0.1685,
      "step": 300
    },
    {
      "epoch": 0.12449799196787148,
      "grad_norm": 0.1880067139863968,
      "learning_rate": 0.00019180722891566266,
      "loss": 0.1861,
      "step": 310
    },
    {
      "epoch": 0.1285140562248996,
      "grad_norm": 0.16637803614139557,
      "learning_rate": 0.00019153949129852745,
      "loss": 0.1216,
      "step": 320
    },
    {
      "epoch": 0.13253012048192772,
      "grad_norm": 0.15783917903900146,
      "learning_rate": 0.00019127175368139224,
      "loss": 0.1684,
      "step": 330
    },
    {
      "epoch": 0.13654618473895583,
      "grad_norm": 0.255624920129776,
      "learning_rate": 0.00019100401606425704,
      "loss": 0.1565,
      "step": 340
    },
    {
      "epoch": 0.14056224899598393,
      "grad_norm": 0.153984934091568,
      "learning_rate": 0.00019073627844712183,
      "loss": 0.1538,
      "step": 350
    },
    {
      "epoch": 0.14457831325301204,
      "grad_norm": 0.10858198255300522,
      "learning_rate": 0.00019046854082998662,
      "loss": 0.1382,
      "step": 360
    },
    {
      "epoch": 0.14859437751004015,
      "grad_norm": 0.18202553689479828,
      "learning_rate": 0.00019020080321285142,
      "loss": 0.167,
      "step": 370
    },
    {
      "epoch": 0.15261044176706828,
      "grad_norm": 0.13070832192897797,
      "learning_rate": 0.0001899330655957162,
      "loss": 0.1608,
      "step": 380
    },
    {
      "epoch": 0.1566265060240964,
      "grad_norm": 0.14221175014972687,
      "learning_rate": 0.000189665327978581,
      "loss": 0.1899,
      "step": 390
    },
    {
      "epoch": 0.1606425702811245,
      "grad_norm": 0.2027258425951004,
      "learning_rate": 0.0001893975903614458,
      "loss": 0.2041,
      "step": 400
    },
    {
      "epoch": 0.1646586345381526,
      "grad_norm": 0.1824778914451599,
      "learning_rate": 0.0001891298527443106,
      "loss": 0.1227,
      "step": 410
    },
    {
      "epoch": 0.1686746987951807,
      "grad_norm": 0.1593070775270462,
      "learning_rate": 0.00018886211512717538,
      "loss": 0.1511,
      "step": 420
    },
    {
      "epoch": 0.17269076305220885,
      "grad_norm": 0.13236947357654572,
      "learning_rate": 0.00018859437751004017,
      "loss": 0.1617,
      "step": 430
    },
    {
      "epoch": 0.17670682730923695,
      "grad_norm": 0.1416393667459488,
      "learning_rate": 0.00018832663989290497,
      "loss": 0.1855,
      "step": 440
    },
    {
      "epoch": 0.18072289156626506,
      "grad_norm": 0.13801097869873047,
      "learning_rate": 0.00018805890227576973,
      "loss": 0.1087,
      "step": 450
    },
    {
      "epoch": 0.18473895582329317,
      "grad_norm": 0.15512816607952118,
      "learning_rate": 0.00018779116465863455,
      "loss": 0.1441,
      "step": 460
    },
    {
      "epoch": 0.18875502008032127,
      "grad_norm": 0.13650621473789215,
      "learning_rate": 0.00018752342704149934,
      "loss": 0.1456,
      "step": 470
    },
    {
      "epoch": 0.1927710843373494,
      "grad_norm": 0.16228686273097992,
      "learning_rate": 0.00018725568942436414,
      "loss": 0.1169,
      "step": 480
    },
    {
      "epoch": 0.19678714859437751,
      "grad_norm": 0.17383013665676117,
      "learning_rate": 0.00018698795180722893,
      "loss": 0.1697,
      "step": 490
    },
    {
      "epoch": 0.20080321285140562,
      "grad_norm": 0.12401378899812698,
      "learning_rate": 0.00018672021419009372,
      "loss": 0.188,
      "step": 500
    },
    {
      "epoch": 0.20481927710843373,
      "grad_norm": 0.09790448099374771,
      "learning_rate": 0.0001864524765729585,
      "loss": 0.1653,
      "step": 510
    },
    {
      "epoch": 0.20883534136546184,
      "grad_norm": 0.12724439799785614,
      "learning_rate": 0.0001861847389558233,
      "loss": 0.1454,
      "step": 520
    },
    {
      "epoch": 0.21285140562248997,
      "grad_norm": 0.20847299695014954,
      "learning_rate": 0.0001859170013386881,
      "loss": 0.1238,
      "step": 530
    },
    {
      "epoch": 0.21686746987951808,
      "grad_norm": 0.20052847266197205,
      "learning_rate": 0.0001856492637215529,
      "loss": 0.1501,
      "step": 540
    },
    {
      "epoch": 0.22088353413654618,
      "grad_norm": 0.15275733172893524,
      "learning_rate": 0.0001853815261044177,
      "loss": 0.1751,
      "step": 550
    },
    {
      "epoch": 0.2248995983935743,
      "grad_norm": 0.14660954475402832,
      "learning_rate": 0.00018511378848728248,
      "loss": 0.14,
      "step": 560
    },
    {
      "epoch": 0.2289156626506024,
      "grad_norm": 0.21540865302085876,
      "learning_rate": 0.00018484605087014725,
      "loss": 0.1439,
      "step": 570
    },
    {
      "epoch": 0.23293172690763053,
      "grad_norm": 0.1386047750711441,
      "learning_rate": 0.00018457831325301204,
      "loss": 0.1192,
      "step": 580
    },
    {
      "epoch": 0.23694779116465864,
      "grad_norm": 0.16499263048171997,
      "learning_rate": 0.00018431057563587686,
      "loss": 0.1817,
      "step": 590
    },
    {
      "epoch": 0.24096385542168675,
      "grad_norm": 0.11850390583276749,
      "learning_rate": 0.00018404283801874165,
      "loss": 0.1207,
      "step": 600
    },
    {
      "epoch": 0.24497991967871485,
      "grad_norm": 0.11292551457881927,
      "learning_rate": 0.00018377510040160644,
      "loss": 0.1515,
      "step": 610
    },
    {
      "epoch": 0.24899598393574296,
      "grad_norm": 0.13298781216144562,
      "learning_rate": 0.00018350736278447124,
      "loss": 0.1459,
      "step": 620
    },
    {
      "epoch": 0.25301204819277107,
      "grad_norm": 0.13340459764003754,
      "learning_rate": 0.000183239625167336,
      "loss": 0.1381,
      "step": 630
    },
    {
      "epoch": 0.2570281124497992,
      "grad_norm": 0.11658608913421631,
      "learning_rate": 0.0001829718875502008,
      "loss": 0.1271,
      "step": 640
    },
    {
      "epoch": 0.26104417670682734,
      "grad_norm": 0.15069490671157837,
      "learning_rate": 0.0001827041499330656,
      "loss": 0.1502,
      "step": 650
    },
    {
      "epoch": 0.26506024096385544,
      "grad_norm": 0.19711509346961975,
      "learning_rate": 0.0001824364123159304,
      "loss": 0.1798,
      "step": 660
    },
    {
      "epoch": 0.26907630522088355,
      "grad_norm": 0.1677546352148056,
      "learning_rate": 0.0001821686746987952,
      "loss": 0.1388,
      "step": 670
    },
    {
      "epoch": 0.27309236947791166,
      "grad_norm": 0.26692306995391846,
      "learning_rate": 0.00018190093708166,
      "loss": 0.2311,
      "step": 680
    },
    {
      "epoch": 0.27710843373493976,
      "grad_norm": 0.15271097421646118,
      "learning_rate": 0.00018163319946452479,
      "loss": 0.1428,
      "step": 690
    },
    {
      "epoch": 0.28112449799196787,
      "grad_norm": 0.11359474062919617,
      "learning_rate": 0.00018136546184738955,
      "loss": 0.1498,
      "step": 700
    },
    {
      "epoch": 0.285140562248996,
      "grad_norm": 0.12054453045129776,
      "learning_rate": 0.00018109772423025434,
      "loss": 0.18,
      "step": 710
    },
    {
      "epoch": 0.2891566265060241,
      "grad_norm": 0.22329112887382507,
      "learning_rate": 0.00018082998661311916,
      "loss": 0.1981,
      "step": 720
    },
    {
      "epoch": 0.2931726907630522,
      "grad_norm": 0.1644359976053238,
      "learning_rate": 0.00018056224899598396,
      "loss": 0.1631,
      "step": 730
    },
    {
      "epoch": 0.2971887550200803,
      "grad_norm": 0.27183371782302856,
      "learning_rate": 0.00018029451137884875,
      "loss": 0.1578,
      "step": 740
    },
    {
      "epoch": 0.30120481927710846,
      "grad_norm": 0.1351560801267624,
      "learning_rate": 0.00018002677376171354,
      "loss": 0.1794,
      "step": 750
    },
    {
      "epoch": 0.30522088353413657,
      "grad_norm": 0.1451903134584427,
      "learning_rate": 0.0001797590361445783,
      "loss": 0.1745,
      "step": 760
    },
    {
      "epoch": 0.3092369477911647,
      "grad_norm": 0.15380533039569855,
      "learning_rate": 0.0001794912985274431,
      "loss": 0.1415,
      "step": 770
    },
    {
      "epoch": 0.3132530120481928,
      "grad_norm": 0.17916899919509888,
      "learning_rate": 0.0001792235609103079,
      "loss": 0.142,
      "step": 780
    },
    {
      "epoch": 0.3172690763052209,
      "grad_norm": 0.11935751885175705,
      "learning_rate": 0.00017895582329317271,
      "loss": 0.1474,
      "step": 790
    },
    {
      "epoch": 0.321285140562249,
      "grad_norm": 0.14896373450756073,
      "learning_rate": 0.0001786880856760375,
      "loss": 0.1409,
      "step": 800
    },
    {
      "epoch": 0.3253012048192771,
      "grad_norm": 0.18507492542266846,
      "learning_rate": 0.0001784203480589023,
      "loss": 0.1332,
      "step": 810
    },
    {
      "epoch": 0.3293172690763052,
      "grad_norm": 0.1385105401277542,
      "learning_rate": 0.00017815261044176707,
      "loss": 0.163,
      "step": 820
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.11274965852499008,
      "learning_rate": 0.00017788487282463186,
      "loss": 0.1533,
      "step": 830
    },
    {
      "epoch": 0.3373493975903614,
      "grad_norm": 0.11774027347564697,
      "learning_rate": 0.00017761713520749665,
      "loss": 0.2024,
      "step": 840
    },
    {
      "epoch": 0.3413654618473896,
      "grad_norm": 0.19043783843517303,
      "learning_rate": 0.00017734939759036144,
      "loss": 0.1457,
      "step": 850
    },
    {
      "epoch": 0.3453815261044177,
      "grad_norm": 0.14338000118732452,
      "learning_rate": 0.00017708165997322626,
      "loss": 0.1536,
      "step": 860
    },
    {
      "epoch": 0.3493975903614458,
      "grad_norm": 0.1685604453086853,
      "learning_rate": 0.00017681392235609106,
      "loss": 0.152,
      "step": 870
    },
    {
      "epoch": 0.3534136546184739,
      "grad_norm": 0.12192444503307343,
      "learning_rate": 0.00017654618473895582,
      "loss": 0.1535,
      "step": 880
    },
    {
      "epoch": 0.357429718875502,
      "grad_norm": 0.15489143133163452,
      "learning_rate": 0.00017627844712182062,
      "loss": 0.1623,
      "step": 890
    },
    {
      "epoch": 0.3614457831325301,
      "grad_norm": 0.1711120307445526,
      "learning_rate": 0.0001760107095046854,
      "loss": 0.1542,
      "step": 900
    },
    {
      "epoch": 0.3654618473895582,
      "grad_norm": 0.1635996699333191,
      "learning_rate": 0.0001757429718875502,
      "loss": 0.1716,
      "step": 910
    },
    {
      "epoch": 0.36947791164658633,
      "grad_norm": 0.22671273350715637,
      "learning_rate": 0.00017547523427041502,
      "loss": 0.1587,
      "step": 920
    },
    {
      "epoch": 0.37349397590361444,
      "grad_norm": 0.14747101068496704,
      "learning_rate": 0.00017520749665327981,
      "loss": 0.1653,
      "step": 930
    },
    {
      "epoch": 0.37751004016064255,
      "grad_norm": 0.24242594838142395,
      "learning_rate": 0.00017493975903614458,
      "loss": 0.1779,
      "step": 940
    },
    {
      "epoch": 0.3815261044176707,
      "grad_norm": 0.13269324600696564,
      "learning_rate": 0.00017467202141900937,
      "loss": 0.1491,
      "step": 950
    },
    {
      "epoch": 0.3855421686746988,
      "grad_norm": 0.13110795617103577,
      "learning_rate": 0.00017440428380187416,
      "loss": 0.1438,
      "step": 960
    },
    {
      "epoch": 0.3895582329317269,
      "grad_norm": 0.2016555815935135,
      "learning_rate": 0.00017413654618473896,
      "loss": 0.1632,
      "step": 970
    },
    {
      "epoch": 0.39357429718875503,
      "grad_norm": 0.15869878232479095,
      "learning_rate": 0.00017386880856760375,
      "loss": 0.1666,
      "step": 980
    },
    {
      "epoch": 0.39759036144578314,
      "grad_norm": 0.2236054688692093,
      "learning_rate": 0.00017360107095046857,
      "loss": 0.1843,
      "step": 990
    },
    {
      "epoch": 0.40160642570281124,
      "grad_norm": 0.178426593542099,
      "learning_rate": 0.00017333333333333334,
      "loss": 0.1504,
      "step": 1000
    },
    {
      "epoch": 0.40562248995983935,
      "grad_norm": 0.17807555198669434,
      "learning_rate": 0.00017306559571619813,
      "loss": 0.1315,
      "step": 1010
    },
    {
      "epoch": 0.40963855421686746,
      "grad_norm": 0.13551288843154907,
      "learning_rate": 0.00017279785809906292,
      "loss": 0.1715,
      "step": 1020
    },
    {
      "epoch": 0.41365461847389556,
      "grad_norm": 0.1560937464237213,
      "learning_rate": 0.00017253012048192771,
      "loss": 0.1578,
      "step": 1030
    },
    {
      "epoch": 0.41767068273092367,
      "grad_norm": 0.16840717196464539,
      "learning_rate": 0.0001722623828647925,
      "loss": 0.1761,
      "step": 1040
    },
    {
      "epoch": 0.42168674698795183,
      "grad_norm": 0.138344407081604,
      "learning_rate": 0.0001719946452476573,
      "loss": 0.1724,
      "step": 1050
    },
    {
      "epoch": 0.42570281124497994,
      "grad_norm": 0.1511981338262558,
      "learning_rate": 0.0001717269076305221,
      "loss": 0.159,
      "step": 1060
    },
    {
      "epoch": 0.42971887550200805,
      "grad_norm": 0.15057812631130219,
      "learning_rate": 0.00017145917001338689,
      "loss": 0.1332,
      "step": 1070
    },
    {
      "epoch": 0.43373493975903615,
      "grad_norm": 0.15636631846427917,
      "learning_rate": 0.00017119143239625168,
      "loss": 0.1749,
      "step": 1080
    },
    {
      "epoch": 0.43775100401606426,
      "grad_norm": 0.09414291381835938,
      "learning_rate": 0.00017092369477911647,
      "loss": 0.1636,
      "step": 1090
    },
    {
      "epoch": 0.44176706827309237,
      "grad_norm": 0.13017037510871887,
      "learning_rate": 0.00017065595716198126,
      "loss": 0.1177,
      "step": 1100
    },
    {
      "epoch": 0.4457831325301205,
      "grad_norm": 0.1808108240365982,
      "learning_rate": 0.00017038821954484606,
      "loss": 0.1479,
      "step": 1110
    },
    {
      "epoch": 0.4497991967871486,
      "grad_norm": 0.17855042219161987,
      "learning_rate": 0.00017012048192771085,
      "loss": 0.1482,
      "step": 1120
    },
    {
      "epoch": 0.4538152610441767,
      "grad_norm": 0.1355665773153305,
      "learning_rate": 0.00016985274431057564,
      "loss": 0.1181,
      "step": 1130
    },
    {
      "epoch": 0.4578313253012048,
      "grad_norm": 0.19207492470741272,
      "learning_rate": 0.00016958500669344044,
      "loss": 0.1501,
      "step": 1140
    },
    {
      "epoch": 0.46184738955823296,
      "grad_norm": 0.13961511850357056,
      "learning_rate": 0.00016931726907630523,
      "loss": 0.1497,
      "step": 1150
    },
    {
      "epoch": 0.46586345381526106,
      "grad_norm": 0.1515282839536667,
      "learning_rate": 0.00016904953145917002,
      "loss": 0.1629,
      "step": 1160
    },
    {
      "epoch": 0.46987951807228917,
      "grad_norm": 0.19519945979118347,
      "learning_rate": 0.00016878179384203481,
      "loss": 0.1312,
      "step": 1170
    },
    {
      "epoch": 0.4738955823293173,
      "grad_norm": 0.1350020170211792,
      "learning_rate": 0.0001685140562248996,
      "loss": 0.1781,
      "step": 1180
    },
    {
      "epoch": 0.4779116465863454,
      "grad_norm": 0.13612115383148193,
      "learning_rate": 0.0001682463186077644,
      "loss": 0.1839,
      "step": 1190
    },
    {
      "epoch": 0.4819277108433735,
      "grad_norm": 0.12584763765335083,
      "learning_rate": 0.0001679785809906292,
      "loss": 0.1415,
      "step": 1200
    },
    {
      "epoch": 0.4859437751004016,
      "grad_norm": 0.16870222985744476,
      "learning_rate": 0.00016771084337349399,
      "loss": 0.1898,
      "step": 1210
    },
    {
      "epoch": 0.4899598393574297,
      "grad_norm": 0.19976180791854858,
      "learning_rate": 0.00016744310575635878,
      "loss": 0.1767,
      "step": 1220
    },
    {
      "epoch": 0.4939759036144578,
      "grad_norm": 0.10171814262866974,
      "learning_rate": 0.00016717536813922357,
      "loss": 0.1563,
      "step": 1230
    },
    {
      "epoch": 0.4979919678714859,
      "grad_norm": 0.16575543582439423,
      "learning_rate": 0.00016690763052208836,
      "loss": 0.1734,
      "step": 1240
    },
    {
      "epoch": 0.5020080321285141,
      "grad_norm": 0.1446106880903244,
      "learning_rate": 0.00016663989290495316,
      "loss": 0.122,
      "step": 1250
    },
    {
      "epoch": 0.5060240963855421,
      "grad_norm": 0.18652759492397308,
      "learning_rate": 0.00016637215528781795,
      "loss": 0.1729,
      "step": 1260
    },
    {
      "epoch": 0.5100401606425703,
      "grad_norm": 0.14889474213123322,
      "learning_rate": 0.00016610441767068274,
      "loss": 0.1719,
      "step": 1270
    },
    {
      "epoch": 0.5140562248995983,
      "grad_norm": 0.13867893815040588,
      "learning_rate": 0.00016583668005354754,
      "loss": 0.1468,
      "step": 1280
    },
    {
      "epoch": 0.5180722891566265,
      "grad_norm": 0.12560537457466125,
      "learning_rate": 0.00016556894243641233,
      "loss": 0.1503,
      "step": 1290
    },
    {
      "epoch": 0.5220883534136547,
      "grad_norm": 0.1710757166147232,
      "learning_rate": 0.00016530120481927712,
      "loss": 0.1649,
      "step": 1300
    },
    {
      "epoch": 0.5261044176706827,
      "grad_norm": 0.126078799366951,
      "learning_rate": 0.0001650334672021419,
      "loss": 0.1231,
      "step": 1310
    },
    {
      "epoch": 0.5301204819277109,
      "grad_norm": 0.18684734404087067,
      "learning_rate": 0.0001647657295850067,
      "loss": 0.1381,
      "step": 1320
    },
    {
      "epoch": 0.5341365461847389,
      "grad_norm": 0.14177273213863373,
      "learning_rate": 0.0001644979919678715,
      "loss": 0.1639,
      "step": 1330
    },
    {
      "epoch": 0.5381526104417671,
      "grad_norm": 0.13033893704414368,
      "learning_rate": 0.0001642302543507363,
      "loss": 0.1286,
      "step": 1340
    },
    {
      "epoch": 0.5421686746987951,
      "grad_norm": 0.2068467140197754,
      "learning_rate": 0.00016396251673360108,
      "loss": 0.1817,
      "step": 1350
    },
    {
      "epoch": 0.5461847389558233,
      "grad_norm": 0.16568896174430847,
      "learning_rate": 0.00016369477911646588,
      "loss": 0.1662,
      "step": 1360
    },
    {
      "epoch": 0.5502008032128514,
      "grad_norm": 0.1512761116027832,
      "learning_rate": 0.00016342704149933067,
      "loss": 0.1404,
      "step": 1370
    },
    {
      "epoch": 0.5542168674698795,
      "grad_norm": 0.17478536069393158,
      "learning_rate": 0.00016315930388219544,
      "loss": 0.133,
      "step": 1380
    },
    {
      "epoch": 0.5582329317269076,
      "grad_norm": 0.16219426691532135,
      "learning_rate": 0.00016289156626506026,
      "loss": 0.1784,
      "step": 1390
    },
    {
      "epoch": 0.5622489959839357,
      "grad_norm": 0.2449439913034439,
      "learning_rate": 0.00016262382864792505,
      "loss": 0.1477,
      "step": 1400
    },
    {
      "epoch": 0.5662650602409639,
      "grad_norm": 0.16494803130626678,
      "learning_rate": 0.00016235609103078984,
      "loss": 0.1566,
      "step": 1410
    },
    {
      "epoch": 0.570281124497992,
      "grad_norm": 0.1676202416419983,
      "learning_rate": 0.00016208835341365463,
      "loss": 0.1217,
      "step": 1420
    },
    {
      "epoch": 0.5742971887550201,
      "grad_norm": 0.15852363407611847,
      "learning_rate": 0.00016182061579651943,
      "loss": 0.1824,
      "step": 1430
    },
    {
      "epoch": 0.5783132530120482,
      "grad_norm": 0.14533358812332153,
      "learning_rate": 0.0001615528781793842,
      "loss": 0.1206,
      "step": 1440
    },
    {
      "epoch": 0.5823293172690763,
      "grad_norm": 0.17694075405597687,
      "learning_rate": 0.00016128514056224899,
      "loss": 0.1744,
      "step": 1450
    },
    {
      "epoch": 0.5863453815261044,
      "grad_norm": 0.20675936341285706,
      "learning_rate": 0.0001610174029451138,
      "loss": 0.183,
      "step": 1460
    },
    {
      "epoch": 0.5903614457831325,
      "grad_norm": 0.1765468716621399,
      "learning_rate": 0.0001607496653279786,
      "loss": 0.1845,
      "step": 1470
    },
    {
      "epoch": 0.5943775100401606,
      "grad_norm": 0.14145928621292114,
      "learning_rate": 0.0001604819277108434,
      "loss": 0.116,
      "step": 1480
    },
    {
      "epoch": 0.5983935742971888,
      "grad_norm": 0.1384068876504898,
      "learning_rate": 0.00016021419009370818,
      "loss": 0.1217,
      "step": 1490
    },
    {
      "epoch": 0.6024096385542169,
      "grad_norm": 0.1646496206521988,
      "learning_rate": 0.00015994645247657295,
      "loss": 0.1713,
      "step": 1500
    },
    {
      "epoch": 0.606425702811245,
      "grad_norm": 0.2034090906381607,
      "learning_rate": 0.00015967871485943774,
      "loss": 0.1345,
      "step": 1510
    },
    {
      "epoch": 0.6104417670682731,
      "grad_norm": 0.18858778476715088,
      "learning_rate": 0.00015941097724230256,
      "loss": 0.1637,
      "step": 1520
    },
    {
      "epoch": 0.6144578313253012,
      "grad_norm": 0.14503659307956696,
      "learning_rate": 0.00015914323962516736,
      "loss": 0.1307,
      "step": 1530
    },
    {
      "epoch": 0.6184738955823293,
      "grad_norm": 0.12711533904075623,
      "learning_rate": 0.00015887550200803215,
      "loss": 0.1089,
      "step": 1540
    },
    {
      "epoch": 0.6224899598393574,
      "grad_norm": 0.1298924833536148,
      "learning_rate": 0.00015860776439089694,
      "loss": 0.1674,
      "step": 1550
    },
    {
      "epoch": 0.6265060240963856,
      "grad_norm": 0.14181561768054962,
      "learning_rate": 0.0001583400267737617,
      "loss": 0.1219,
      "step": 1560
    },
    {
      "epoch": 0.6305220883534136,
      "grad_norm": 0.13411593437194824,
      "learning_rate": 0.0001580722891566265,
      "loss": 0.1702,
      "step": 1570
    },
    {
      "epoch": 0.6345381526104418,
      "grad_norm": 0.19297632575035095,
      "learning_rate": 0.0001578045515394913,
      "loss": 0.1305,
      "step": 1580
    },
    {
      "epoch": 0.6385542168674698,
      "grad_norm": 0.16957364976406097,
      "learning_rate": 0.0001575368139223561,
      "loss": 0.1523,
      "step": 1590
    },
    {
      "epoch": 0.642570281124498,
      "grad_norm": 0.15720896422863007,
      "learning_rate": 0.0001572690763052209,
      "loss": 0.2011,
      "step": 1600
    },
    {
      "epoch": 0.6465863453815262,
      "grad_norm": 0.12194468826055527,
      "learning_rate": 0.0001570013386880857,
      "loss": 0.1165,
      "step": 1610
    },
    {
      "epoch": 0.6506024096385542,
      "grad_norm": 0.1913793683052063,
      "learning_rate": 0.00015673360107095046,
      "loss": 0.1456,
      "step": 1620
    },
    {
      "epoch": 0.6546184738955824,
      "grad_norm": 0.1666683852672577,
      "learning_rate": 0.00015646586345381526,
      "loss": 0.1367,
      "step": 1630
    },
    {
      "epoch": 0.6586345381526104,
      "grad_norm": 0.1570734977722168,
      "learning_rate": 0.00015619812583668005,
      "loss": 0.138,
      "step": 1640
    },
    {
      "epoch": 0.6626506024096386,
      "grad_norm": 0.13522537052631378,
      "learning_rate": 0.00015593038821954484,
      "loss": 0.1418,
      "step": 1650
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.11988849937915802,
      "learning_rate": 0.00015566265060240966,
      "loss": 0.1508,
      "step": 1660
    },
    {
      "epoch": 0.6706827309236948,
      "grad_norm": 0.1615239381790161,
      "learning_rate": 0.00015539491298527445,
      "loss": 0.1391,
      "step": 1670
    },
    {
      "epoch": 0.6746987951807228,
      "grad_norm": 0.14989010989665985,
      "learning_rate": 0.00015512717536813922,
      "loss": 0.1495,
      "step": 1680
    },
    {
      "epoch": 0.678714859437751,
      "grad_norm": 0.1414366364479065,
      "learning_rate": 0.000154859437751004,
      "loss": 0.1504,
      "step": 1690
    },
    {
      "epoch": 0.6827309236947792,
      "grad_norm": 0.13701817393302917,
      "learning_rate": 0.0001545917001338688,
      "loss": 0.1503,
      "step": 1700
    },
    {
      "epoch": 0.6867469879518072,
      "grad_norm": 0.17706459760665894,
      "learning_rate": 0.0001543239625167336,
      "loss": 0.1664,
      "step": 1710
    },
    {
      "epoch": 0.6907630522088354,
      "grad_norm": 0.1573748141527176,
      "learning_rate": 0.00015405622489959842,
      "loss": 0.1322,
      "step": 1720
    },
    {
      "epoch": 0.6947791164658634,
      "grad_norm": 0.18719801306724548,
      "learning_rate": 0.0001537884872824632,
      "loss": 0.1762,
      "step": 1730
    },
    {
      "epoch": 0.6987951807228916,
      "grad_norm": 0.1446426659822464,
      "learning_rate": 0.00015352074966532798,
      "loss": 0.1365,
      "step": 1740
    },
    {
      "epoch": 0.7028112449799196,
      "grad_norm": 0.14664840698242188,
      "learning_rate": 0.00015325301204819277,
      "loss": 0.1422,
      "step": 1750
    },
    {
      "epoch": 0.7068273092369478,
      "grad_norm": 0.14992567896842957,
      "learning_rate": 0.00015298527443105756,
      "loss": 0.1377,
      "step": 1760
    },
    {
      "epoch": 0.7108433734939759,
      "grad_norm": 0.20165078341960907,
      "learning_rate": 0.00015271753681392236,
      "loss": 0.1707,
      "step": 1770
    },
    {
      "epoch": 0.714859437751004,
      "grad_norm": 0.18316949903964996,
      "learning_rate": 0.00015244979919678715,
      "loss": 0.1636,
      "step": 1780
    },
    {
      "epoch": 0.7188755020080321,
      "grad_norm": 0.11523500084877014,
      "learning_rate": 0.00015218206157965197,
      "loss": 0.1467,
      "step": 1790
    },
    {
      "epoch": 0.7228915662650602,
      "grad_norm": 0.1483432948589325,
      "learning_rate": 0.00015191432396251673,
      "loss": 0.1797,
      "step": 1800
    },
    {
      "epoch": 0.7269076305220884,
      "grad_norm": 0.1268533617258072,
      "learning_rate": 0.00015164658634538153,
      "loss": 0.1509,
      "step": 1810
    },
    {
      "epoch": 0.7309236947791165,
      "grad_norm": 0.12657074630260468,
      "learning_rate": 0.00015137884872824632,
      "loss": 0.1242,
      "step": 1820
    },
    {
      "epoch": 0.7349397590361446,
      "grad_norm": 0.20312917232513428,
      "learning_rate": 0.0001511111111111111,
      "loss": 0.1585,
      "step": 1830
    },
    {
      "epoch": 0.7389558232931727,
      "grad_norm": 0.1361571103334427,
      "learning_rate": 0.0001508433734939759,
      "loss": 0.175,
      "step": 1840
    },
    {
      "epoch": 0.7429718875502008,
      "grad_norm": 0.16797319054603577,
      "learning_rate": 0.0001505756358768407,
      "loss": 0.1171,
      "step": 1850
    },
    {
      "epoch": 0.7469879518072289,
      "grad_norm": 0.22230452299118042,
      "learning_rate": 0.0001503078982597055,
      "loss": 0.1407,
      "step": 1860
    },
    {
      "epoch": 0.751004016064257,
      "grad_norm": 0.15182189643383026,
      "learning_rate": 0.00015004016064257028,
      "loss": 0.1229,
      "step": 1870
    },
    {
      "epoch": 0.7550200803212851,
      "grad_norm": 0.13179554045200348,
      "learning_rate": 0.00014977242302543508,
      "loss": 0.1799,
      "step": 1880
    },
    {
      "epoch": 0.7590361445783133,
      "grad_norm": 0.22870773077011108,
      "learning_rate": 0.00014950468540829987,
      "loss": 0.159,
      "step": 1890
    },
    {
      "epoch": 0.7630522088353414,
      "grad_norm": 0.13298849761486053,
      "learning_rate": 0.00014923694779116466,
      "loss": 0.1523,
      "step": 1900
    },
    {
      "epoch": 0.7670682730923695,
      "grad_norm": 0.13293488323688507,
      "learning_rate": 0.00014896921017402946,
      "loss": 0.14,
      "step": 1910
    },
    {
      "epoch": 0.7710843373493976,
      "grad_norm": 0.12157325446605682,
      "learning_rate": 0.00014870147255689425,
      "loss": 0.152,
      "step": 1920
    },
    {
      "epoch": 0.7751004016064257,
      "grad_norm": 0.2803887128829956,
      "learning_rate": 0.00014843373493975904,
      "loss": 0.1765,
      "step": 1930
    },
    {
      "epoch": 0.7791164658634538,
      "grad_norm": 0.27045729756355286,
      "learning_rate": 0.00014816599732262383,
      "loss": 0.1545,
      "step": 1940
    },
    {
      "epoch": 0.7831325301204819,
      "grad_norm": 0.174331933259964,
      "learning_rate": 0.00014789825970548863,
      "loss": 0.1636,
      "step": 1950
    },
    {
      "epoch": 0.7871485943775101,
      "grad_norm": 0.14737986028194427,
      "learning_rate": 0.00014763052208835342,
      "loss": 0.0956,
      "step": 1960
    },
    {
      "epoch": 0.7911646586345381,
      "grad_norm": 0.157323956489563,
      "learning_rate": 0.0001473627844712182,
      "loss": 0.1434,
      "step": 1970
    },
    {
      "epoch": 0.7951807228915663,
      "grad_norm": 0.23479250073432922,
      "learning_rate": 0.000147095046854083,
      "loss": 0.1215,
      "step": 1980
    },
    {
      "epoch": 0.7991967871485943,
      "grad_norm": 0.17147290706634521,
      "learning_rate": 0.0001468273092369478,
      "loss": 0.1272,
      "step": 1990
    },
    {
      "epoch": 0.8032128514056225,
      "grad_norm": 0.146162748336792,
      "learning_rate": 0.0001465595716198126,
      "loss": 0.17,
      "step": 2000
    },
    {
      "epoch": 0.8072289156626506,
      "grad_norm": 0.15895162522792816,
      "learning_rate": 0.00014629183400267738,
      "loss": 0.1342,
      "step": 2010
    },
    {
      "epoch": 0.8112449799196787,
      "grad_norm": 0.1418740600347519,
      "learning_rate": 0.00014602409638554218,
      "loss": 0.1433,
      "step": 2020
    },
    {
      "epoch": 0.8152610441767069,
      "grad_norm": 0.13832919299602509,
      "learning_rate": 0.00014575635876840697,
      "loss": 0.1251,
      "step": 2030
    },
    {
      "epoch": 0.8192771084337349,
      "grad_norm": 0.1706903725862503,
      "learning_rate": 0.00014548862115127176,
      "loss": 0.1435,
      "step": 2040
    },
    {
      "epoch": 0.8232931726907631,
      "grad_norm": 0.1428888589143753,
      "learning_rate": 0.00014522088353413655,
      "loss": 0.1476,
      "step": 2050
    },
    {
      "epoch": 0.8273092369477911,
      "grad_norm": 0.16879449784755707,
      "learning_rate": 0.00014495314591700135,
      "loss": 0.181,
      "step": 2060
    },
    {
      "epoch": 0.8313253012048193,
      "grad_norm": 0.14744998514652252,
      "learning_rate": 0.00014468540829986614,
      "loss": 0.1475,
      "step": 2070
    },
    {
      "epoch": 0.8353413654618473,
      "grad_norm": 0.15174032747745514,
      "learning_rate": 0.00014441767068273093,
      "loss": 0.1922,
      "step": 2080
    },
    {
      "epoch": 0.8393574297188755,
      "grad_norm": 0.2093653529882431,
      "learning_rate": 0.00014414993306559573,
      "loss": 0.148,
      "step": 2090
    },
    {
      "epoch": 0.8433734939759037,
      "grad_norm": 0.1728537678718567,
      "learning_rate": 0.00014388219544846052,
      "loss": 0.1377,
      "step": 2100
    },
    {
      "epoch": 0.8473895582329317,
      "grad_norm": 0.14544805884361267,
      "learning_rate": 0.0001436144578313253,
      "loss": 0.1633,
      "step": 2110
    },
    {
      "epoch": 0.8514056224899599,
      "grad_norm": 0.22361646592617035,
      "learning_rate": 0.0001433467202141901,
      "loss": 0.1344,
      "step": 2120
    },
    {
      "epoch": 0.8554216867469879,
      "grad_norm": 0.1855863779783249,
      "learning_rate": 0.0001430789825970549,
      "loss": 0.107,
      "step": 2130
    },
    {
      "epoch": 0.8594377510040161,
      "grad_norm": 0.140947163105011,
      "learning_rate": 0.0001428112449799197,
      "loss": 0.1452,
      "step": 2140
    },
    {
      "epoch": 0.8634538152610441,
      "grad_norm": 0.13547131419181824,
      "learning_rate": 0.00014254350736278448,
      "loss": 0.1057,
      "step": 2150
    },
    {
      "epoch": 0.8674698795180723,
      "grad_norm": 0.15266266465187073,
      "learning_rate": 0.00014227576974564928,
      "loss": 0.1291,
      "step": 2160
    },
    {
      "epoch": 0.8714859437751004,
      "grad_norm": 0.14990490674972534,
      "learning_rate": 0.00014200803212851407,
      "loss": 0.1133,
      "step": 2170
    },
    {
      "epoch": 0.8755020080321285,
      "grad_norm": 0.1931552141904831,
      "learning_rate": 0.00014174029451137883,
      "loss": 0.1395,
      "step": 2180
    },
    {
      "epoch": 0.8795180722891566,
      "grad_norm": 0.20146194100379944,
      "learning_rate": 0.00014147255689424365,
      "loss": 0.2296,
      "step": 2190
    },
    {
      "epoch": 0.8835341365461847,
      "grad_norm": 0.18594665825366974,
      "learning_rate": 0.00014120481927710845,
      "loss": 0.1458,
      "step": 2200
    },
    {
      "epoch": 0.8875502008032129,
      "grad_norm": 0.16860772669315338,
      "learning_rate": 0.00014093708165997324,
      "loss": 0.1118,
      "step": 2210
    },
    {
      "epoch": 0.891566265060241,
      "grad_norm": 0.14327372610569,
      "learning_rate": 0.00014066934404283803,
      "loss": 0.1304,
      "step": 2220
    },
    {
      "epoch": 0.8955823293172691,
      "grad_norm": 0.19004997611045837,
      "learning_rate": 0.00014040160642570283,
      "loss": 0.1699,
      "step": 2230
    },
    {
      "epoch": 0.8995983935742972,
      "grad_norm": 0.15001191198825836,
      "learning_rate": 0.0001401338688085676,
      "loss": 0.119,
      "step": 2240
    },
    {
      "epoch": 0.9036144578313253,
      "grad_norm": 0.12003504484891891,
      "learning_rate": 0.00013986613119143238,
      "loss": 0.173,
      "step": 2250
    },
    {
      "epoch": 0.9076305220883534,
      "grad_norm": 0.1998170018196106,
      "learning_rate": 0.0001395983935742972,
      "loss": 0.1523,
      "step": 2260
    },
    {
      "epoch": 0.9116465863453815,
      "grad_norm": 0.12704575061798096,
      "learning_rate": 0.000139330655957162,
      "loss": 0.1245,
      "step": 2270
    },
    {
      "epoch": 0.9156626506024096,
      "grad_norm": 0.13816171884536743,
      "learning_rate": 0.0001390629183400268,
      "loss": 0.161,
      "step": 2280
    },
    {
      "epoch": 0.9196787148594378,
      "grad_norm": 0.1560342013835907,
      "learning_rate": 0.00013879518072289158,
      "loss": 0.1445,
      "step": 2290
    },
    {
      "epoch": 0.9236947791164659,
      "grad_norm": 0.1514168679714203,
      "learning_rate": 0.00013852744310575635,
      "loss": 0.1582,
      "step": 2300
    },
    {
      "epoch": 0.927710843373494,
      "grad_norm": 0.1497749239206314,
      "learning_rate": 0.00013825970548862114,
      "loss": 0.1073,
      "step": 2310
    },
    {
      "epoch": 0.9317269076305221,
      "grad_norm": 0.12062451243400574,
      "learning_rate": 0.00013799196787148596,
      "loss": 0.166,
      "step": 2320
    },
    {
      "epoch": 0.9357429718875502,
      "grad_norm": 0.1867484450340271,
      "learning_rate": 0.00013772423025435075,
      "loss": 0.1412,
      "step": 2330
    },
    {
      "epoch": 0.9397590361445783,
      "grad_norm": 0.10965792834758759,
      "learning_rate": 0.00013745649263721555,
      "loss": 0.1069,
      "step": 2340
    },
    {
      "epoch": 0.9437751004016064,
      "grad_norm": 0.15761560201644897,
      "learning_rate": 0.00013718875502008034,
      "loss": 0.1519,
      "step": 2350
    },
    {
      "epoch": 0.9477911646586346,
      "grad_norm": 0.166342094540596,
      "learning_rate": 0.0001369210174029451,
      "loss": 0.1457,
      "step": 2360
    },
    {
      "epoch": 0.9518072289156626,
      "grad_norm": 0.14710386097431183,
      "learning_rate": 0.0001366532797858099,
      "loss": 0.1206,
      "step": 2370
    },
    {
      "epoch": 0.9558232931726908,
      "grad_norm": 0.1507560908794403,
      "learning_rate": 0.0001363855421686747,
      "loss": 0.1837,
      "step": 2380
    },
    {
      "epoch": 0.9598393574297188,
      "grad_norm": 0.1197100579738617,
      "learning_rate": 0.0001361178045515395,
      "loss": 0.1258,
      "step": 2390
    },
    {
      "epoch": 0.963855421686747,
      "grad_norm": 0.18566913902759552,
      "learning_rate": 0.0001358500669344043,
      "loss": 0.1398,
      "step": 2400
    },
    {
      "epoch": 0.9678714859437751,
      "grad_norm": 0.14627191424369812,
      "learning_rate": 0.0001355823293172691,
      "loss": 0.1262,
      "step": 2410
    },
    {
      "epoch": 0.9718875502008032,
      "grad_norm": 0.12952326238155365,
      "learning_rate": 0.00013531459170013386,
      "loss": 0.157,
      "step": 2420
    },
    {
      "epoch": 0.9759036144578314,
      "grad_norm": 0.18679475784301758,
      "learning_rate": 0.00013504685408299865,
      "loss": 0.1961,
      "step": 2430
    },
    {
      "epoch": 0.9799196787148594,
      "grad_norm": 0.1517753005027771,
      "learning_rate": 0.00013477911646586345,
      "loss": 0.1532,
      "step": 2440
    },
    {
      "epoch": 0.9839357429718876,
      "grad_norm": 0.1255030333995819,
      "learning_rate": 0.00013451137884872824,
      "loss": 0.1724,
      "step": 2450
    },
    {
      "epoch": 0.9879518072289156,
      "grad_norm": 0.18230731785297394,
      "learning_rate": 0.00013424364123159306,
      "loss": 0.1234,
      "step": 2460
    },
    {
      "epoch": 0.9919678714859438,
      "grad_norm": 0.16073386371135712,
      "learning_rate": 0.00013397590361445785,
      "loss": 0.1214,
      "step": 2470
    },
    {
      "epoch": 0.9959839357429718,
      "grad_norm": 0.14152446389198303,
      "learning_rate": 0.00013370816599732262,
      "loss": 0.1497,
      "step": 2480
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.2896365821361542,
      "learning_rate": 0.0001334404283801874,
      "loss": 0.1469,
      "step": 2490
    },
    {
      "epoch": 1.0040160642570282,
      "grad_norm": 0.15488608181476593,
      "learning_rate": 0.0001331726907630522,
      "loss": 0.1564,
      "step": 2500
    },
    {
      "epoch": 1.0080321285140563,
      "grad_norm": 0.13880683481693268,
      "learning_rate": 0.000132904953145917,
      "loss": 0.1708,
      "step": 2510
    },
    {
      "epoch": 1.0120481927710843,
      "grad_norm": 0.15283089876174927,
      "learning_rate": 0.00013263721552878182,
      "loss": 0.115,
      "step": 2520
    },
    {
      "epoch": 1.0160642570281124,
      "grad_norm": 0.1424121856689453,
      "learning_rate": 0.0001323694779116466,
      "loss": 0.1218,
      "step": 2530
    },
    {
      "epoch": 1.0200803212851406,
      "grad_norm": 0.1506858915090561,
      "learning_rate": 0.00013210174029451138,
      "loss": 0.0964,
      "step": 2540
    },
    {
      "epoch": 1.0240963855421688,
      "grad_norm": 0.17300383746623993,
      "learning_rate": 0.00013183400267737617,
      "loss": 0.1456,
      "step": 2550
    },
    {
      "epoch": 1.0281124497991967,
      "grad_norm": 0.27756428718566895,
      "learning_rate": 0.00013156626506024096,
      "loss": 0.1571,
      "step": 2560
    },
    {
      "epoch": 1.0321285140562249,
      "grad_norm": 0.22316865622997284,
      "learning_rate": 0.00013129852744310575,
      "loss": 0.1437,
      "step": 2570
    },
    {
      "epoch": 1.036144578313253,
      "grad_norm": 0.21816349029541016,
      "learning_rate": 0.00013103078982597055,
      "loss": 0.1454,
      "step": 2580
    },
    {
      "epoch": 1.0401606425702812,
      "grad_norm": 0.1572958379983902,
      "learning_rate": 0.00013076305220883537,
      "loss": 0.1235,
      "step": 2590
    },
    {
      "epoch": 1.0441767068273093,
      "grad_norm": 0.1709272265434265,
      "learning_rate": 0.00013049531459170013,
      "loss": 0.1485,
      "step": 2600
    },
    {
      "epoch": 1.0481927710843373,
      "grad_norm": 0.11161566525697708,
      "learning_rate": 0.00013022757697456493,
      "loss": 0.1608,
      "step": 2610
    },
    {
      "epoch": 1.0522088353413654,
      "grad_norm": 0.14918021857738495,
      "learning_rate": 0.00012995983935742972,
      "loss": 0.1601,
      "step": 2620
    },
    {
      "epoch": 1.0562248995983936,
      "grad_norm": 0.16277940571308136,
      "learning_rate": 0.0001296921017402945,
      "loss": 0.1217,
      "step": 2630
    },
    {
      "epoch": 1.0602409638554218,
      "grad_norm": 0.14191500842571259,
      "learning_rate": 0.0001294243641231593,
      "loss": 0.1166,
      "step": 2640
    },
    {
      "epoch": 1.0642570281124497,
      "grad_norm": 0.17753192782402039,
      "learning_rate": 0.0001291566265060241,
      "loss": 0.1475,
      "step": 2650
    },
    {
      "epoch": 1.0682730923694779,
      "grad_norm": 0.15497922897338867,
      "learning_rate": 0.00012888888888888892,
      "loss": 0.1497,
      "step": 2660
    },
    {
      "epoch": 1.072289156626506,
      "grad_norm": 0.21750064194202423,
      "learning_rate": 0.00012862115127175368,
      "loss": 0.1677,
      "step": 2670
    },
    {
      "epoch": 1.0763052208835342,
      "grad_norm": 0.19109725952148438,
      "learning_rate": 0.00012835341365461848,
      "loss": 0.1543,
      "step": 2680
    },
    {
      "epoch": 1.0803212851405624,
      "grad_norm": 0.20604902505874634,
      "learning_rate": 0.00012808567603748327,
      "loss": 0.1338,
      "step": 2690
    },
    {
      "epoch": 1.0843373493975903,
      "grad_norm": 0.18202432990074158,
      "learning_rate": 0.00012781793842034806,
      "loss": 0.1467,
      "step": 2700
    },
    {
      "epoch": 1.0883534136546185,
      "grad_norm": 0.13923484086990356,
      "learning_rate": 0.00012755020080321285,
      "loss": 0.1102,
      "step": 2710
    },
    {
      "epoch": 1.0923694779116466,
      "grad_norm": 0.19392818212509155,
      "learning_rate": 0.00012728246318607767,
      "loss": 0.1619,
      "step": 2720
    },
    {
      "epoch": 1.0963855421686748,
      "grad_norm": 0.14915211498737335,
      "learning_rate": 0.00012701472556894244,
      "loss": 0.134,
      "step": 2730
    },
    {
      "epoch": 1.1004016064257027,
      "grad_norm": 0.18493516743183136,
      "learning_rate": 0.00012674698795180723,
      "loss": 0.1389,
      "step": 2740
    },
    {
      "epoch": 1.104417670682731,
      "grad_norm": 0.20316942036151886,
      "learning_rate": 0.00012647925033467202,
      "loss": 0.1166,
      "step": 2750
    },
    {
      "epoch": 1.108433734939759,
      "grad_norm": 0.17061415314674377,
      "learning_rate": 0.00012621151271753682,
      "loss": 0.165,
      "step": 2760
    },
    {
      "epoch": 1.1124497991967872,
      "grad_norm": 0.18297722935676575,
      "learning_rate": 0.0001259437751004016,
      "loss": 0.1558,
      "step": 2770
    },
    {
      "epoch": 1.1164658634538154,
      "grad_norm": 0.10655515640974045,
      "learning_rate": 0.0001256760374832664,
      "loss": 0.0991,
      "step": 2780
    },
    {
      "epoch": 1.1204819277108433,
      "grad_norm": 0.14245198667049408,
      "learning_rate": 0.0001254082998661312,
      "loss": 0.1007,
      "step": 2790
    },
    {
      "epoch": 1.1244979919678715,
      "grad_norm": 0.19315022230148315,
      "learning_rate": 0.000125140562248996,
      "loss": 0.1249,
      "step": 2800
    },
    {
      "epoch": 1.1285140562248996,
      "grad_norm": 0.2217005342245102,
      "learning_rate": 0.00012487282463186078,
      "loss": 0.1528,
      "step": 2810
    },
    {
      "epoch": 1.1325301204819278,
      "grad_norm": 0.1694340854883194,
      "learning_rate": 0.00012460508701472557,
      "loss": 0.1292,
      "step": 2820
    },
    {
      "epoch": 1.1365461847389557,
      "grad_norm": 0.12561142444610596,
      "learning_rate": 0.00012433734939759037,
      "loss": 0.1995,
      "step": 2830
    },
    {
      "epoch": 1.140562248995984,
      "grad_norm": 0.15094922482967377,
      "learning_rate": 0.00012406961178045516,
      "loss": 0.112,
      "step": 2840
    },
    {
      "epoch": 1.144578313253012,
      "grad_norm": 0.27049872279167175,
      "learning_rate": 0.00012380187416331995,
      "loss": 0.1451,
      "step": 2850
    },
    {
      "epoch": 1.1485943775100402,
      "grad_norm": 0.18201567232608795,
      "learning_rate": 0.00012353413654618475,
      "loss": 0.1562,
      "step": 2860
    },
    {
      "epoch": 1.1526104417670684,
      "grad_norm": 0.14878207445144653,
      "learning_rate": 0.00012326639892904954,
      "loss": 0.1197,
      "step": 2870
    },
    {
      "epoch": 1.1566265060240963,
      "grad_norm": 0.18539054691791534,
      "learning_rate": 0.00012299866131191433,
      "loss": 0.1199,
      "step": 2880
    },
    {
      "epoch": 1.1606425702811245,
      "grad_norm": 0.16187722980976105,
      "learning_rate": 0.00012273092369477912,
      "loss": 0.1099,
      "step": 2890
    },
    {
      "epoch": 1.1646586345381527,
      "grad_norm": 0.14780065417289734,
      "learning_rate": 0.00012246318607764392,
      "loss": 0.1476,
      "step": 2900
    },
    {
      "epoch": 1.1686746987951806,
      "grad_norm": 0.21337831020355225,
      "learning_rate": 0.0001221954484605087,
      "loss": 0.1268,
      "step": 2910
    },
    {
      "epoch": 1.1726907630522088,
      "grad_norm": 0.16148394346237183,
      "learning_rate": 0.00012192771084337352,
      "loss": 0.1223,
      "step": 2920
    },
    {
      "epoch": 1.176706827309237,
      "grad_norm": 0.1734725683927536,
      "learning_rate": 0.0001216599732262383,
      "loss": 0.144,
      "step": 2930
    },
    {
      "epoch": 1.180722891566265,
      "grad_norm": 0.1308937817811966,
      "learning_rate": 0.00012139223560910309,
      "loss": 0.1266,
      "step": 2940
    },
    {
      "epoch": 1.1847389558232932,
      "grad_norm": 0.18327507376670837,
      "learning_rate": 0.00012112449799196788,
      "loss": 0.1564,
      "step": 2950
    },
    {
      "epoch": 1.1887550200803212,
      "grad_norm": 0.17814582586288452,
      "learning_rate": 0.00012085676037483266,
      "loss": 0.1501,
      "step": 2960
    },
    {
      "epoch": 1.1927710843373494,
      "grad_norm": 0.15094706416130066,
      "learning_rate": 0.00012058902275769745,
      "loss": 0.0997,
      "step": 2970
    },
    {
      "epoch": 1.1967871485943775,
      "grad_norm": 0.19581453502178192,
      "learning_rate": 0.00012032128514056225,
      "loss": 0.128,
      "step": 2980
    },
    {
      "epoch": 1.2008032128514057,
      "grad_norm": 0.1867039054632187,
      "learning_rate": 0.00012005354752342705,
      "loss": 0.1322,
      "step": 2990
    },
    {
      "epoch": 1.2048192771084336,
      "grad_norm": 0.13208380341529846,
      "learning_rate": 0.00011978580990629185,
      "loss": 0.1647,
      "step": 3000
    },
    {
      "epoch": 1.2088353413654618,
      "grad_norm": 0.18255217373371124,
      "learning_rate": 0.00011951807228915664,
      "loss": 0.1298,
      "step": 3010
    },
    {
      "epoch": 1.21285140562249,
      "grad_norm": 0.16834118962287903,
      "learning_rate": 0.00011925033467202143,
      "loss": 0.1351,
      "step": 3020
    },
    {
      "epoch": 1.216867469879518,
      "grad_norm": 0.2570718228816986,
      "learning_rate": 0.00011898259705488621,
      "loss": 0.1558,
      "step": 3030
    },
    {
      "epoch": 1.2208835341365463,
      "grad_norm": 0.12555043399333954,
      "learning_rate": 0.000118714859437751,
      "loss": 0.1434,
      "step": 3040
    },
    {
      "epoch": 1.2248995983935742,
      "grad_norm": 0.12492936849594116,
      "learning_rate": 0.0001184471218206158,
      "loss": 0.1358,
      "step": 3050
    },
    {
      "epoch": 1.2289156626506024,
      "grad_norm": 0.1717592179775238,
      "learning_rate": 0.0001181793842034806,
      "loss": 0.1216,
      "step": 3060
    },
    {
      "epoch": 1.2329317269076305,
      "grad_norm": 0.14566436409950256,
      "learning_rate": 0.0001179116465863454,
      "loss": 0.1132,
      "step": 3070
    },
    {
      "epoch": 1.2369477911646587,
      "grad_norm": 0.1912757158279419,
      "learning_rate": 0.00011764390896921019,
      "loss": 0.1854,
      "step": 3080
    },
    {
      "epoch": 1.2409638554216866,
      "grad_norm": 0.28542560338974,
      "learning_rate": 0.00011737617135207497,
      "loss": 0.1555,
      "step": 3090
    },
    {
      "epoch": 1.2449799196787148,
      "grad_norm": 0.1919213980436325,
      "learning_rate": 0.00011710843373493976,
      "loss": 0.133,
      "step": 3100
    },
    {
      "epoch": 1.248995983935743,
      "grad_norm": 0.10498251020908356,
      "learning_rate": 0.00011684069611780455,
      "loss": 0.1641,
      "step": 3110
    },
    {
      "epoch": 1.2530120481927711,
      "grad_norm": 0.1941412389278412,
      "learning_rate": 0.00011657295850066936,
      "loss": 0.1342,
      "step": 3120
    },
    {
      "epoch": 1.2570281124497993,
      "grad_norm": 0.12066426128149033,
      "learning_rate": 0.00011630522088353415,
      "loss": 0.1188,
      "step": 3130
    },
    {
      "epoch": 1.2610441767068274,
      "grad_norm": 0.1659376472234726,
      "learning_rate": 0.00011603748326639894,
      "loss": 0.1686,
      "step": 3140
    },
    {
      "epoch": 1.2650602409638554,
      "grad_norm": 0.30784931778907776,
      "learning_rate": 0.00011576974564926372,
      "loss": 0.1442,
      "step": 3150
    },
    {
      "epoch": 1.2690763052208835,
      "grad_norm": 0.21327243745326996,
      "learning_rate": 0.00011550200803212852,
      "loss": 0.1477,
      "step": 3160
    },
    {
      "epoch": 1.2730923694779117,
      "grad_norm": 0.17780277132987976,
      "learning_rate": 0.00011523427041499331,
      "loss": 0.151,
      "step": 3170
    },
    {
      "epoch": 1.2771084337349397,
      "grad_norm": 0.19065438210964203,
      "learning_rate": 0.00011496653279785809,
      "loss": 0.1436,
      "step": 3180
    },
    {
      "epoch": 1.2811244979919678,
      "grad_norm": 0.14468441903591156,
      "learning_rate": 0.00011469879518072291,
      "loss": 0.1577,
      "step": 3190
    },
    {
      "epoch": 1.285140562248996,
      "grad_norm": 0.16372884809970856,
      "learning_rate": 0.0001144310575635877,
      "loss": 0.1348,
      "step": 3200
    },
    {
      "epoch": 1.2891566265060241,
      "grad_norm": 0.14766521751880646,
      "learning_rate": 0.00011416331994645248,
      "loss": 0.1321,
      "step": 3210
    },
    {
      "epoch": 1.2931726907630523,
      "grad_norm": 0.25123316049575806,
      "learning_rate": 0.00011389558232931727,
      "loss": 0.1508,
      "step": 3220
    },
    {
      "epoch": 1.2971887550200802,
      "grad_norm": 0.15832874178886414,
      "learning_rate": 0.00011362784471218207,
      "loss": 0.1431,
      "step": 3230
    },
    {
      "epoch": 1.3012048192771084,
      "grad_norm": 0.1672006994485855,
      "learning_rate": 0.00011336010709504685,
      "loss": 0.1739,
      "step": 3240
    },
    {
      "epoch": 1.3052208835341366,
      "grad_norm": 0.16545653343200684,
      "learning_rate": 0.00011309236947791164,
      "loss": 0.1762,
      "step": 3250
    },
    {
      "epoch": 1.3092369477911647,
      "grad_norm": 0.19127050042152405,
      "learning_rate": 0.00011282463186077646,
      "loss": 0.1309,
      "step": 3260
    },
    {
      "epoch": 1.3132530120481927,
      "grad_norm": 0.1318175494670868,
      "learning_rate": 0.00011255689424364124,
      "loss": 0.1522,
      "step": 3270
    },
    {
      "epoch": 1.3172690763052208,
      "grad_norm": 0.20138031244277954,
      "learning_rate": 0.00011228915662650603,
      "loss": 0.1209,
      "step": 3280
    },
    {
      "epoch": 1.321285140562249,
      "grad_norm": 0.2324230819940567,
      "learning_rate": 0.00011202141900937082,
      "loss": 0.1644,
      "step": 3290
    },
    {
      "epoch": 1.3253012048192772,
      "grad_norm": 0.15821698307991028,
      "learning_rate": 0.00011175368139223562,
      "loss": 0.1585,
      "step": 3300
    },
    {
      "epoch": 1.3293172690763053,
      "grad_norm": 0.21826936304569244,
      "learning_rate": 0.0001114859437751004,
      "loss": 0.1514,
      "step": 3310
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.2185892015695572,
      "learning_rate": 0.00011121820615796522,
      "loss": 0.1463,
      "step": 3320
    },
    {
      "epoch": 1.3373493975903614,
      "grad_norm": 0.14535585045814514,
      "learning_rate": 0.00011095046854083,
      "loss": 0.1266,
      "step": 3330
    },
    {
      "epoch": 1.3413654618473896,
      "grad_norm": 0.18171152472496033,
      "learning_rate": 0.00011068273092369479,
      "loss": 0.1231,
      "step": 3340
    },
    {
      "epoch": 1.3453815261044177,
      "grad_norm": 0.21634504199028015,
      "learning_rate": 0.00011041499330655958,
      "loss": 0.135,
      "step": 3350
    },
    {
      "epoch": 1.3493975903614457,
      "grad_norm": 0.14685456454753876,
      "learning_rate": 0.00011014725568942437,
      "loss": 0.125,
      "step": 3360
    },
    {
      "epoch": 1.3534136546184738,
      "grad_norm": 0.18230842053890228,
      "learning_rate": 0.00010987951807228915,
      "loss": 0.127,
      "step": 3370
    },
    {
      "epoch": 1.357429718875502,
      "grad_norm": 0.16558107733726501,
      "learning_rate": 0.00010961178045515395,
      "loss": 0.131,
      "step": 3380
    },
    {
      "epoch": 1.3614457831325302,
      "grad_norm": 0.1907179206609726,
      "learning_rate": 0.00010934404283801875,
      "loss": 0.1356,
      "step": 3390
    },
    {
      "epoch": 1.3654618473895583,
      "grad_norm": 0.16415512561798096,
      "learning_rate": 0.00010907630522088354,
      "loss": 0.1289,
      "step": 3400
    },
    {
      "epoch": 1.3694779116465863,
      "grad_norm": 0.18924210965633392,
      "learning_rate": 0.00010880856760374834,
      "loss": 0.1473,
      "step": 3410
    },
    {
      "epoch": 1.3734939759036144,
      "grad_norm": 0.26109713315963745,
      "learning_rate": 0.00010854082998661313,
      "loss": 0.1373,
      "step": 3420
    },
    {
      "epoch": 1.3775100401606426,
      "grad_norm": 0.16015630960464478,
      "learning_rate": 0.00010827309236947791,
      "loss": 0.1084,
      "step": 3430
    },
    {
      "epoch": 1.3815261044176708,
      "grad_norm": 0.21128132939338684,
      "learning_rate": 0.0001080053547523427,
      "loss": 0.1684,
      "step": 3440
    },
    {
      "epoch": 1.3855421686746987,
      "grad_norm": 0.3108733594417572,
      "learning_rate": 0.0001077376171352075,
      "loss": 0.1582,
      "step": 3450
    },
    {
      "epoch": 1.3895582329317269,
      "grad_norm": 0.16797246038913727,
      "learning_rate": 0.0001074698795180723,
      "loss": 0.1177,
      "step": 3460
    },
    {
      "epoch": 1.393574297188755,
      "grad_norm": 0.17965492606163025,
      "learning_rate": 0.0001072021419009371,
      "loss": 0.1363,
      "step": 3470
    },
    {
      "epoch": 1.3975903614457832,
      "grad_norm": 0.12836307287216187,
      "learning_rate": 0.00010693440428380189,
      "loss": 0.1558,
      "step": 3480
    },
    {
      "epoch": 1.4016064257028114,
      "grad_norm": 0.2487487494945526,
      "learning_rate": 0.00010666666666666667,
      "loss": 0.1273,
      "step": 3490
    },
    {
      "epoch": 1.4056224899598393,
      "grad_norm": 0.18895423412322998,
      "learning_rate": 0.00010639892904953146,
      "loss": 0.1188,
      "step": 3500
    },
    {
      "epoch": 1.4096385542168675,
      "grad_norm": 0.128241166472435,
      "learning_rate": 0.00010613119143239625,
      "loss": 0.1373,
      "step": 3510
    },
    {
      "epoch": 1.4136546184738956,
      "grad_norm": 0.1973068118095398,
      "learning_rate": 0.00010586345381526106,
      "loss": 0.1794,
      "step": 3520
    },
    {
      "epoch": 1.4176706827309236,
      "grad_norm": 0.27155518531799316,
      "learning_rate": 0.00010559571619812585,
      "loss": 0.1118,
      "step": 3530
    },
    {
      "epoch": 1.4216867469879517,
      "grad_norm": 0.17903192341327667,
      "learning_rate": 0.00010532797858099064,
      "loss": 0.1446,
      "step": 3540
    },
    {
      "epoch": 1.4257028112449799,
      "grad_norm": 0.259446918964386,
      "learning_rate": 0.00010506024096385542,
      "loss": 0.159,
      "step": 3550
    },
    {
      "epoch": 1.429718875502008,
      "grad_norm": 0.15819472074508667,
      "learning_rate": 0.00010479250334672022,
      "loss": 0.1249,
      "step": 3560
    },
    {
      "epoch": 1.4337349397590362,
      "grad_norm": 0.27315041422843933,
      "learning_rate": 0.00010452476572958501,
      "loss": 0.164,
      "step": 3570
    },
    {
      "epoch": 1.4377510040160644,
      "grad_norm": 0.2665278911590576,
      "learning_rate": 0.0001042570281124498,
      "loss": 0.1294,
      "step": 3580
    },
    {
      "epoch": 1.4417670682730923,
      "grad_norm": 0.17992761731147766,
      "learning_rate": 0.00010398929049531461,
      "loss": 0.1351,
      "step": 3590
    },
    {
      "epoch": 1.4457831325301205,
      "grad_norm": 0.1809673011302948,
      "learning_rate": 0.0001037215528781794,
      "loss": 0.1169,
      "step": 3600
    },
    {
      "epoch": 1.4497991967871486,
      "grad_norm": 0.17306733131408691,
      "learning_rate": 0.00010345381526104418,
      "loss": 0.142,
      "step": 3610
    },
    {
      "epoch": 1.4538152610441766,
      "grad_norm": 0.14153257012367249,
      "learning_rate": 0.00010318607764390897,
      "loss": 0.1332,
      "step": 3620
    },
    {
      "epoch": 1.4578313253012047,
      "grad_norm": 0.14748862385749817,
      "learning_rate": 0.00010291834002677377,
      "loss": 0.1307,
      "step": 3630
    },
    {
      "epoch": 1.461847389558233,
      "grad_norm": 0.12375565618276596,
      "learning_rate": 0.00010265060240963856,
      "loss": 0.1195,
      "step": 3640
    },
    {
      "epoch": 1.465863453815261,
      "grad_norm": 0.2544165849685669,
      "learning_rate": 0.00010238286479250334,
      "loss": 0.1567,
      "step": 3650
    },
    {
      "epoch": 1.4698795180722892,
      "grad_norm": 0.21385222673416138,
      "learning_rate": 0.00010211512717536816,
      "loss": 0.1554,
      "step": 3660
    },
    {
      "epoch": 1.4738955823293174,
      "grad_norm": 0.19984127581119537,
      "learning_rate": 0.00010184738955823294,
      "loss": 0.1309,
      "step": 3670
    },
    {
      "epoch": 1.4779116465863453,
      "grad_norm": 0.22910240292549133,
      "learning_rate": 0.00010157965194109773,
      "loss": 0.1621,
      "step": 3680
    },
    {
      "epoch": 1.4819277108433735,
      "grad_norm": 0.19153115153312683,
      "learning_rate": 0.00010131191432396252,
      "loss": 0.1278,
      "step": 3690
    },
    {
      "epoch": 1.4859437751004017,
      "grad_norm": 0.23210231959819794,
      "learning_rate": 0.00010104417670682732,
      "loss": 0.1363,
      "step": 3700
    },
    {
      "epoch": 1.4899598393574296,
      "grad_norm": 0.17028269171714783,
      "learning_rate": 0.0001007764390896921,
      "loss": 0.1537,
      "step": 3710
    },
    {
      "epoch": 1.4939759036144578,
      "grad_norm": 0.14400945603847504,
      "learning_rate": 0.00010050870147255691,
      "loss": 0.1443,
      "step": 3720
    },
    {
      "epoch": 1.497991967871486,
      "grad_norm": 0.19261908531188965,
      "learning_rate": 0.0001002409638554217,
      "loss": 0.1603,
      "step": 3730
    },
    {
      "epoch": 1.502008032128514,
      "grad_norm": 0.15482087433338165,
      "learning_rate": 9.997322623828649e-05,
      "loss": 0.1206,
      "step": 3740
    },
    {
      "epoch": 1.5060240963855422,
      "grad_norm": 0.25347667932510376,
      "learning_rate": 9.970548862115128e-05,
      "loss": 0.1666,
      "step": 3750
    },
    {
      "epoch": 1.5100401606425704,
      "grad_norm": 0.12997983396053314,
      "learning_rate": 9.943775100401607e-05,
      "loss": 0.1345,
      "step": 3760
    },
    {
      "epoch": 1.5140562248995983,
      "grad_norm": 0.24585337936878204,
      "learning_rate": 9.917001338688087e-05,
      "loss": 0.1609,
      "step": 3770
    },
    {
      "epoch": 1.5180722891566265,
      "grad_norm": 0.1254454404115677,
      "learning_rate": 9.890227576974566e-05,
      "loss": 0.1242,
      "step": 3780
    },
    {
      "epoch": 1.5220883534136547,
      "grad_norm": 0.20383788645267487,
      "learning_rate": 9.863453815261045e-05,
      "loss": 0.1262,
      "step": 3790
    },
    {
      "epoch": 1.5261044176706826,
      "grad_norm": 0.1976134181022644,
      "learning_rate": 9.836680053547523e-05,
      "loss": 0.1732,
      "step": 3800
    },
    {
      "epoch": 1.5301204819277108,
      "grad_norm": 0.12883096933364868,
      "learning_rate": 9.809906291834004e-05,
      "loss": 0.1635,
      "step": 3810
    },
    {
      "epoch": 1.534136546184739,
      "grad_norm": 0.15595632791519165,
      "learning_rate": 9.783132530120483e-05,
      "loss": 0.1216,
      "step": 3820
    },
    {
      "epoch": 1.538152610441767,
      "grad_norm": 0.16921819746494293,
      "learning_rate": 9.756358768406961e-05,
      "loss": 0.1186,
      "step": 3830
    },
    {
      "epoch": 1.5421686746987953,
      "grad_norm": 0.2259533405303955,
      "learning_rate": 9.729585006693441e-05,
      "loss": 0.1326,
      "step": 3840
    },
    {
      "epoch": 1.5461847389558234,
      "grad_norm": 0.1810373216867447,
      "learning_rate": 9.702811244979921e-05,
      "loss": 0.1077,
      "step": 3850
    },
    {
      "epoch": 1.5502008032128514,
      "grad_norm": 0.24765290319919586,
      "learning_rate": 9.676037483266399e-05,
      "loss": 0.195,
      "step": 3860
    },
    {
      "epoch": 1.5542168674698795,
      "grad_norm": 0.1348242312669754,
      "learning_rate": 9.64926372155288e-05,
      "loss": 0.1442,
      "step": 3870
    },
    {
      "epoch": 1.5582329317269075,
      "grad_norm": 0.17932187020778656,
      "learning_rate": 9.622489959839359e-05,
      "loss": 0.1355,
      "step": 3880
    },
    {
      "epoch": 1.5622489959839356,
      "grad_norm": 0.15837928652763367,
      "learning_rate": 9.595716198125837e-05,
      "loss": 0.1183,
      "step": 3890
    },
    {
      "epoch": 1.5662650602409638,
      "grad_norm": 0.19903044402599335,
      "learning_rate": 9.568942436412316e-05,
      "loss": 0.1147,
      "step": 3900
    },
    {
      "epoch": 1.570281124497992,
      "grad_norm": 0.18729278445243835,
      "learning_rate": 9.542168674698796e-05,
      "loss": 0.1339,
      "step": 3910
    },
    {
      "epoch": 1.5742971887550201,
      "grad_norm": 0.2028847634792328,
      "learning_rate": 9.515394912985274e-05,
      "loss": 0.13,
      "step": 3920
    },
    {
      "epoch": 1.5783132530120483,
      "grad_norm": 0.20082999765872955,
      "learning_rate": 9.488621151271754e-05,
      "loss": 0.1775,
      "step": 3930
    },
    {
      "epoch": 1.5823293172690764,
      "grad_norm": 0.20409099757671356,
      "learning_rate": 9.461847389558234e-05,
      "loss": 0.1384,
      "step": 3940
    },
    {
      "epoch": 1.5863453815261044,
      "grad_norm": 0.12436037510633469,
      "learning_rate": 9.435073627844712e-05,
      "loss": 0.1426,
      "step": 3950
    },
    {
      "epoch": 1.5903614457831325,
      "grad_norm": 0.14609982073307037,
      "learning_rate": 9.408299866131192e-05,
      "loss": 0.1343,
      "step": 3960
    },
    {
      "epoch": 1.5943775100401605,
      "grad_norm": 0.25272834300994873,
      "learning_rate": 9.381526104417672e-05,
      "loss": 0.1626,
      "step": 3970
    },
    {
      "epoch": 1.5983935742971886,
      "grad_norm": 0.22374533116817474,
      "learning_rate": 9.35475234270415e-05,
      "loss": 0.1205,
      "step": 3980
    },
    {
      "epoch": 1.6024096385542168,
      "grad_norm": 0.32622072100639343,
      "learning_rate": 9.32797858099063e-05,
      "loss": 0.1908,
      "step": 3990
    },
    {
      "epoch": 1.606425702811245,
      "grad_norm": 0.16899271309375763,
      "learning_rate": 9.301204819277109e-05,
      "loss": 0.1205,
      "step": 4000
    },
    {
      "epoch": 1.6104417670682731,
      "grad_norm": 0.2512485682964325,
      "learning_rate": 9.274431057563588e-05,
      "loss": 0.1213,
      "step": 4010
    },
    {
      "epoch": 1.6144578313253013,
      "grad_norm": 0.18154560029506683,
      "learning_rate": 9.247657295850067e-05,
      "loss": 0.133,
      "step": 4020
    },
    {
      "epoch": 1.6184738955823295,
      "grad_norm": 0.18893298506736755,
      "learning_rate": 9.220883534136546e-05,
      "loss": 0.1539,
      "step": 4030
    },
    {
      "epoch": 1.6224899598393574,
      "grad_norm": 0.200680673122406,
      "learning_rate": 9.194109772423026e-05,
      "loss": 0.1541,
      "step": 4040
    },
    {
      "epoch": 1.6265060240963856,
      "grad_norm": 0.16842839121818542,
      "learning_rate": 9.167336010709505e-05,
      "loss": 0.1296,
      "step": 4050
    },
    {
      "epoch": 1.6305220883534135,
      "grad_norm": 0.16439437866210938,
      "learning_rate": 9.140562248995984e-05,
      "loss": 0.1441,
      "step": 4060
    },
    {
      "epoch": 1.6345381526104417,
      "grad_norm": 0.13823194801807404,
      "learning_rate": 9.113788487282464e-05,
      "loss": 0.1307,
      "step": 4070
    },
    {
      "epoch": 1.6385542168674698,
      "grad_norm": 0.42154940962791443,
      "learning_rate": 9.087014725568943e-05,
      "loss": 0.169,
      "step": 4080
    },
    {
      "epoch": 1.642570281124498,
      "grad_norm": 0.1915043145418167,
      "learning_rate": 9.060240963855422e-05,
      "loss": 0.1479,
      "step": 4090
    },
    {
      "epoch": 1.6465863453815262,
      "grad_norm": 0.1559356451034546,
      "learning_rate": 9.033467202141901e-05,
      "loss": 0.1261,
      "step": 4100
    },
    {
      "epoch": 1.6506024096385543,
      "grad_norm": 0.1884460598230362,
      "learning_rate": 9.006693440428381e-05,
      "loss": 0.1525,
      "step": 4110
    },
    {
      "epoch": 1.6546184738955825,
      "grad_norm": 0.1313854306936264,
      "learning_rate": 8.97991967871486e-05,
      "loss": 0.1371,
      "step": 4120
    },
    {
      "epoch": 1.6586345381526104,
      "grad_norm": 0.15326757729053497,
      "learning_rate": 8.953145917001339e-05,
      "loss": 0.1418,
      "step": 4130
    },
    {
      "epoch": 1.6626506024096386,
      "grad_norm": 0.21625608205795288,
      "learning_rate": 8.926372155287819e-05,
      "loss": 0.18,
      "step": 4140
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.17097055912017822,
      "learning_rate": 8.899598393574298e-05,
      "loss": 0.1372,
      "step": 4150
    },
    {
      "epoch": 1.6706827309236947,
      "grad_norm": 0.14303554594516754,
      "learning_rate": 8.872824631860777e-05,
      "loss": 0.1417,
      "step": 4160
    },
    {
      "epoch": 1.6746987951807228,
      "grad_norm": 0.2219858169555664,
      "learning_rate": 8.846050870147256e-05,
      "loss": 0.1806,
      "step": 4170
    },
    {
      "epoch": 1.678714859437751,
      "grad_norm": 0.1472509503364563,
      "learning_rate": 8.819277108433736e-05,
      "loss": 0.1449,
      "step": 4180
    },
    {
      "epoch": 1.6827309236947792,
      "grad_norm": 0.19740797579288483,
      "learning_rate": 8.792503346720215e-05,
      "loss": 0.1548,
      "step": 4190
    },
    {
      "epoch": 1.6867469879518073,
      "grad_norm": 0.1707202047109604,
      "learning_rate": 8.765729585006693e-05,
      "loss": 0.1238,
      "step": 4200
    },
    {
      "epoch": 1.6907630522088355,
      "grad_norm": 0.1830512434244156,
      "learning_rate": 8.738955823293174e-05,
      "loss": 0.1307,
      "step": 4210
    },
    {
      "epoch": 1.6947791164658634,
      "grad_norm": 0.20115584135055542,
      "learning_rate": 8.712182061579653e-05,
      "loss": 0.1361,
      "step": 4220
    },
    {
      "epoch": 1.6987951807228916,
      "grad_norm": 0.20671546459197998,
      "learning_rate": 8.685408299866131e-05,
      "loss": 0.1593,
      "step": 4230
    },
    {
      "epoch": 1.7028112449799195,
      "grad_norm": 0.2301642745733261,
      "learning_rate": 8.658634538152611e-05,
      "loss": 0.1487,
      "step": 4240
    },
    {
      "epoch": 1.7068273092369477,
      "grad_norm": 0.1359463483095169,
      "learning_rate": 8.631860776439091e-05,
      "loss": 0.1404,
      "step": 4250
    },
    {
      "epoch": 1.7108433734939759,
      "grad_norm": 0.20168784260749817,
      "learning_rate": 8.605087014725569e-05,
      "loss": 0.1364,
      "step": 4260
    },
    {
      "epoch": 1.714859437751004,
      "grad_norm": 0.18973694741725922,
      "learning_rate": 8.578313253012049e-05,
      "loss": 0.2157,
      "step": 4270
    },
    {
      "epoch": 1.7188755020080322,
      "grad_norm": 0.16974715888500214,
      "learning_rate": 8.551539491298529e-05,
      "loss": 0.175,
      "step": 4280
    },
    {
      "epoch": 1.7228915662650603,
      "grad_norm": 0.26222851872444153,
      "learning_rate": 8.524765729585006e-05,
      "loss": 0.1208,
      "step": 4290
    },
    {
      "epoch": 1.7269076305220885,
      "grad_norm": 0.18089623749256134,
      "learning_rate": 8.497991967871486e-05,
      "loss": 0.1132,
      "step": 4300
    },
    {
      "epoch": 1.7309236947791165,
      "grad_norm": 0.14195984601974487,
      "learning_rate": 8.471218206157966e-05,
      "loss": 0.1421,
      "step": 4310
    },
    {
      "epoch": 1.7349397590361446,
      "grad_norm": 0.14582061767578125,
      "learning_rate": 8.444444444444444e-05,
      "loss": 0.1387,
      "step": 4320
    },
    {
      "epoch": 1.7389558232931726,
      "grad_norm": 0.2108626812696457,
      "learning_rate": 8.417670682730924e-05,
      "loss": 0.1408,
      "step": 4330
    },
    {
      "epoch": 1.7429718875502007,
      "grad_norm": 0.2116033434867859,
      "learning_rate": 8.390896921017404e-05,
      "loss": 0.1707,
      "step": 4340
    },
    {
      "epoch": 1.7469879518072289,
      "grad_norm": 0.1664642095565796,
      "learning_rate": 8.364123159303882e-05,
      "loss": 0.1483,
      "step": 4350
    },
    {
      "epoch": 1.751004016064257,
      "grad_norm": 0.21778520941734314,
      "learning_rate": 8.337349397590361e-05,
      "loss": 0.1295,
      "step": 4360
    },
    {
      "epoch": 1.7550200803212852,
      "grad_norm": 0.20410871505737305,
      "learning_rate": 8.310575635876842e-05,
      "loss": 0.1015,
      "step": 4370
    },
    {
      "epoch": 1.7590361445783134,
      "grad_norm": 0.1346856653690338,
      "learning_rate": 8.28380187416332e-05,
      "loss": 0.1352,
      "step": 4380
    },
    {
      "epoch": 1.7630522088353415,
      "grad_norm": 0.17492873966693878,
      "learning_rate": 8.257028112449799e-05,
      "loss": 0.1381,
      "step": 4390
    },
    {
      "epoch": 1.7670682730923695,
      "grad_norm": 0.17806659638881683,
      "learning_rate": 8.230254350736279e-05,
      "loss": 0.106,
      "step": 4400
    },
    {
      "epoch": 1.7710843373493976,
      "grad_norm": 0.1585056483745575,
      "learning_rate": 8.203480589022758e-05,
      "loss": 0.1636,
      "step": 4410
    },
    {
      "epoch": 1.7751004016064256,
      "grad_norm": 0.18293212354183197,
      "learning_rate": 8.176706827309237e-05,
      "loss": 0.1543,
      "step": 4420
    },
    {
      "epoch": 1.7791164658634537,
      "grad_norm": 0.17278693616390228,
      "learning_rate": 8.149933065595716e-05,
      "loss": 0.1434,
      "step": 4430
    },
    {
      "epoch": 1.783132530120482,
      "grad_norm": 0.10528110712766647,
      "learning_rate": 8.123159303882196e-05,
      "loss": 0.1185,
      "step": 4440
    },
    {
      "epoch": 1.78714859437751,
      "grad_norm": 0.16729895770549774,
      "learning_rate": 8.096385542168675e-05,
      "loss": 0.1211,
      "step": 4450
    },
    {
      "epoch": 1.7911646586345382,
      "grad_norm": 0.2083914577960968,
      "learning_rate": 8.069611780455154e-05,
      "loss": 0.1039,
      "step": 4460
    },
    {
      "epoch": 1.7951807228915664,
      "grad_norm": 0.26285091042518616,
      "learning_rate": 8.042838018741634e-05,
      "loss": 0.1364,
      "step": 4470
    },
    {
      "epoch": 1.7991967871485943,
      "grad_norm": 0.11997857689857483,
      "learning_rate": 8.016064257028113e-05,
      "loss": 0.1024,
      "step": 4480
    },
    {
      "epoch": 1.8032128514056225,
      "grad_norm": 0.14192724227905273,
      "learning_rate": 7.989290495314592e-05,
      "loss": 0.1175,
      "step": 4490
    },
    {
      "epoch": 1.8072289156626506,
      "grad_norm": 0.2811968922615051,
      "learning_rate": 7.962516733601071e-05,
      "loss": 0.1736,
      "step": 4500
    },
    {
      "epoch": 1.8112449799196786,
      "grad_norm": 0.26831746101379395,
      "learning_rate": 7.93574297188755e-05,
      "loss": 0.1357,
      "step": 4510
    },
    {
      "epoch": 1.8152610441767068,
      "grad_norm": 0.17374174296855927,
      "learning_rate": 7.90896921017403e-05,
      "loss": 0.1566,
      "step": 4520
    },
    {
      "epoch": 1.819277108433735,
      "grad_norm": 0.17909011244773865,
      "learning_rate": 7.882195448460509e-05,
      "loss": 0.1209,
      "step": 4530
    },
    {
      "epoch": 1.823293172690763,
      "grad_norm": 0.175545334815979,
      "learning_rate": 7.855421686746989e-05,
      "loss": 0.1548,
      "step": 4540
    },
    {
      "epoch": 1.8273092369477912,
      "grad_norm": 0.2745325565338135,
      "learning_rate": 7.828647925033468e-05,
      "loss": 0.1267,
      "step": 4550
    },
    {
      "epoch": 1.8313253012048194,
      "grad_norm": 0.15273348987102509,
      "learning_rate": 7.801874163319947e-05,
      "loss": 0.1007,
      "step": 4560
    },
    {
      "epoch": 1.8353413654618473,
      "grad_norm": 0.13916122913360596,
      "learning_rate": 7.775100401606426e-05,
      "loss": 0.1403,
      "step": 4570
    },
    {
      "epoch": 1.8393574297188755,
      "grad_norm": 0.18686078488826752,
      "learning_rate": 7.748326639892906e-05,
      "loss": 0.1699,
      "step": 4580
    },
    {
      "epoch": 1.8433734939759037,
      "grad_norm": 0.17156653106212616,
      "learning_rate": 7.721552878179385e-05,
      "loss": 0.1647,
      "step": 4590
    },
    {
      "epoch": 1.8473895582329316,
      "grad_norm": 0.13731186091899872,
      "learning_rate": 7.694779116465863e-05,
      "loss": 0.1045,
      "step": 4600
    },
    {
      "epoch": 1.8514056224899598,
      "grad_norm": 0.22466249763965607,
      "learning_rate": 7.668005354752343e-05,
      "loss": 0.1489,
      "step": 4610
    },
    {
      "epoch": 1.855421686746988,
      "grad_norm": 0.15261612832546234,
      "learning_rate": 7.641231593038823e-05,
      "loss": 0.1086,
      "step": 4620
    },
    {
      "epoch": 1.859437751004016,
      "grad_norm": 0.1947622448205948,
      "learning_rate": 7.614457831325301e-05,
      "loss": 0.1806,
      "step": 4630
    },
    {
      "epoch": 1.8634538152610443,
      "grad_norm": 0.1407877504825592,
      "learning_rate": 7.587684069611781e-05,
      "loss": 0.1517,
      "step": 4640
    },
    {
      "epoch": 1.8674698795180724,
      "grad_norm": 0.1766843944787979,
      "learning_rate": 7.56091030789826e-05,
      "loss": 0.0975,
      "step": 4650
    },
    {
      "epoch": 1.8714859437751004,
      "grad_norm": 0.15164868533611298,
      "learning_rate": 7.534136546184739e-05,
      "loss": 0.1435,
      "step": 4660
    },
    {
      "epoch": 1.8755020080321285,
      "grad_norm": 0.17264656722545624,
      "learning_rate": 7.507362784471219e-05,
      "loss": 0.1488,
      "step": 4670
    },
    {
      "epoch": 1.8795180722891565,
      "grad_norm": 0.18380454182624817,
      "learning_rate": 7.480589022757698e-05,
      "loss": 0.1523,
      "step": 4680
    },
    {
      "epoch": 1.8835341365461846,
      "grad_norm": 0.1351190060377121,
      "learning_rate": 7.453815261044176e-05,
      "loss": 0.1754,
      "step": 4690
    },
    {
      "epoch": 1.8875502008032128,
      "grad_norm": 0.16505523025989532,
      "learning_rate": 7.427041499330656e-05,
      "loss": 0.1492,
      "step": 4700
    },
    {
      "epoch": 1.891566265060241,
      "grad_norm": 0.1686716377735138,
      "learning_rate": 7.400267737617136e-05,
      "loss": 0.1777,
      "step": 4710
    },
    {
      "epoch": 1.895582329317269,
      "grad_norm": 0.1770784556865692,
      "learning_rate": 7.373493975903614e-05,
      "loss": 0.1205,
      "step": 4720
    },
    {
      "epoch": 1.8995983935742973,
      "grad_norm": 0.15939219295978546,
      "learning_rate": 7.346720214190094e-05,
      "loss": 0.1535,
      "step": 4730
    },
    {
      "epoch": 1.9036144578313254,
      "grad_norm": 0.2080140858888626,
      "learning_rate": 7.319946452476574e-05,
      "loss": 0.1236,
      "step": 4740
    },
    {
      "epoch": 1.9076305220883534,
      "grad_norm": 0.233356773853302,
      "learning_rate": 7.293172690763052e-05,
      "loss": 0.149,
      "step": 4750
    },
    {
      "epoch": 1.9116465863453815,
      "grad_norm": 0.15943922102451324,
      "learning_rate": 7.266398929049531e-05,
      "loss": 0.1383,
      "step": 4760
    },
    {
      "epoch": 1.9156626506024095,
      "grad_norm": 0.13181129097938538,
      "learning_rate": 7.239625167336012e-05,
      "loss": 0.1587,
      "step": 4770
    },
    {
      "epoch": 1.9196787148594376,
      "grad_norm": 0.2000816911458969,
      "learning_rate": 7.21285140562249e-05,
      "loss": 0.1194,
      "step": 4780
    },
    {
      "epoch": 1.9236947791164658,
      "grad_norm": 0.2986850142478943,
      "learning_rate": 7.186077643908969e-05,
      "loss": 0.1496,
      "step": 4790
    },
    {
      "epoch": 1.927710843373494,
      "grad_norm": 0.11272822320461273,
      "learning_rate": 7.159303882195448e-05,
      "loss": 0.1581,
      "step": 4800
    },
    {
      "epoch": 1.9317269076305221,
      "grad_norm": 0.12851731479167938,
      "learning_rate": 7.132530120481928e-05,
      "loss": 0.1355,
      "step": 4810
    },
    {
      "epoch": 1.9357429718875503,
      "grad_norm": 0.1922610104084015,
      "learning_rate": 7.105756358768407e-05,
      "loss": 0.1211,
      "step": 4820
    },
    {
      "epoch": 1.9397590361445785,
      "grad_norm": 0.1590125411748886,
      "learning_rate": 7.078982597054886e-05,
      "loss": 0.1363,
      "step": 4830
    },
    {
      "epoch": 1.9437751004016064,
      "grad_norm": 0.1757252961397171,
      "learning_rate": 7.052208835341366e-05,
      "loss": 0.1435,
      "step": 4840
    },
    {
      "epoch": 1.9477911646586346,
      "grad_norm": 0.19221031665802002,
      "learning_rate": 7.025435073627845e-05,
      "loss": 0.1388,
      "step": 4850
    },
    {
      "epoch": 1.9518072289156625,
      "grad_norm": 0.21753966808319092,
      "learning_rate": 6.998661311914324e-05,
      "loss": 0.1278,
      "step": 4860
    },
    {
      "epoch": 1.9558232931726907,
      "grad_norm": 0.3838387727737427,
      "learning_rate": 6.971887550200803e-05,
      "loss": 0.1697,
      "step": 4870
    },
    {
      "epoch": 1.9598393574297188,
      "grad_norm": 0.1661163866519928,
      "learning_rate": 6.945113788487283e-05,
      "loss": 0.1169,
      "step": 4880
    },
    {
      "epoch": 1.963855421686747,
      "grad_norm": 0.2257753312587738,
      "learning_rate": 6.918340026773762e-05,
      "loss": 0.1661,
      "step": 4890
    },
    {
      "epoch": 1.9678714859437751,
      "grad_norm": 0.20087777078151703,
      "learning_rate": 6.891566265060241e-05,
      "loss": 0.1403,
      "step": 4900
    },
    {
      "epoch": 1.9718875502008033,
      "grad_norm": 0.1670527309179306,
      "learning_rate": 6.86479250334672e-05,
      "loss": 0.1564,
      "step": 4910
    },
    {
      "epoch": 1.9759036144578315,
      "grad_norm": 0.2123689353466034,
      "learning_rate": 6.8380187416332e-05,
      "loss": 0.1427,
      "step": 4920
    },
    {
      "epoch": 1.9799196787148594,
      "grad_norm": 0.2319646179676056,
      "learning_rate": 6.811244979919679e-05,
      "loss": 0.1437,
      "step": 4930
    },
    {
      "epoch": 1.9839357429718876,
      "grad_norm": 0.2281056046485901,
      "learning_rate": 6.784471218206158e-05,
      "loss": 0.1822,
      "step": 4940
    },
    {
      "epoch": 1.9879518072289155,
      "grad_norm": 0.1788872480392456,
      "learning_rate": 6.757697456492638e-05,
      "loss": 0.142,
      "step": 4950
    },
    {
      "epoch": 1.9919678714859437,
      "grad_norm": 0.20706428587436676,
      "learning_rate": 6.730923694779117e-05,
      "loss": 0.1532,
      "step": 4960
    },
    {
      "epoch": 1.9959839357429718,
      "grad_norm": 0.19379444420337677,
      "learning_rate": 6.704149933065596e-05,
      "loss": 0.1437,
      "step": 4970
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.1838580071926117,
      "learning_rate": 6.677376171352076e-05,
      "loss": 0.09,
      "step": 4980
    }
  ],
  "logging_steps": 10,
  "max_steps": 7470,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 7.404089861839258e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
