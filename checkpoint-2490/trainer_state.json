{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 2490,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004016064257028112,
      "grad_norm": 1.3410128355026245,
      "learning_rate": 0.00019983935742971887,
      "loss": 2.6242,
      "step": 10
    },
    {
      "epoch": 0.008032128514056224,
      "grad_norm": 0.17546769976615906,
      "learning_rate": 0.00019957161981258367,
      "loss": 0.1865,
      "step": 20
    },
    {
      "epoch": 0.012048192771084338,
      "grad_norm": 0.2884160578250885,
      "learning_rate": 0.00019930388219544846,
      "loss": 0.1674,
      "step": 30
    },
    {
      "epoch": 0.01606425702811245,
      "grad_norm": 0.16600263118743896,
      "learning_rate": 0.00019903614457831325,
      "loss": 0.1931,
      "step": 40
    },
    {
      "epoch": 0.020080321285140562,
      "grad_norm": 0.22165589034557343,
      "learning_rate": 0.00019876840696117805,
      "loss": 0.2165,
      "step": 50
    },
    {
      "epoch": 0.024096385542168676,
      "grad_norm": 0.1574738323688507,
      "learning_rate": 0.00019850066934404287,
      "loss": 0.1706,
      "step": 60
    },
    {
      "epoch": 0.028112449799196786,
      "grad_norm": 0.17505013942718506,
      "learning_rate": 0.00019823293172690763,
      "loss": 0.1834,
      "step": 70
    },
    {
      "epoch": 0.0321285140562249,
      "grad_norm": 0.17651118338108063,
      "learning_rate": 0.00019796519410977242,
      "loss": 0.1879,
      "step": 80
    },
    {
      "epoch": 0.03614457831325301,
      "grad_norm": 0.14281855523586273,
      "learning_rate": 0.00019769745649263722,
      "loss": 0.1622,
      "step": 90
    },
    {
      "epoch": 0.040160642570281124,
      "grad_norm": 0.1657632738351822,
      "learning_rate": 0.000197429718875502,
      "loss": 0.1685,
      "step": 100
    },
    {
      "epoch": 0.04417670682730924,
      "grad_norm": 0.16495706140995026,
      "learning_rate": 0.0001971619812583668,
      "loss": 0.1576,
      "step": 110
    },
    {
      "epoch": 0.04819277108433735,
      "grad_norm": 0.2072693407535553,
      "learning_rate": 0.00019689424364123162,
      "loss": 0.2029,
      "step": 120
    },
    {
      "epoch": 0.05220883534136546,
      "grad_norm": 0.18284757435321808,
      "learning_rate": 0.00019662650602409642,
      "loss": 0.1569,
      "step": 130
    },
    {
      "epoch": 0.05622489959839357,
      "grad_norm": 0.14073553681373596,
      "learning_rate": 0.00019635876840696118,
      "loss": 0.1549,
      "step": 140
    },
    {
      "epoch": 0.060240963855421686,
      "grad_norm": 0.12602190673351288,
      "learning_rate": 0.00019609103078982597,
      "loss": 0.1727,
      "step": 150
    },
    {
      "epoch": 0.0642570281124498,
      "grad_norm": 0.1342284232378006,
      "learning_rate": 0.00019582329317269077,
      "loss": 0.1565,
      "step": 160
    },
    {
      "epoch": 0.06827309236947791,
      "grad_norm": 0.22000163793563843,
      "learning_rate": 0.00019555555555555556,
      "loss": 0.146,
      "step": 170
    },
    {
      "epoch": 0.07228915662650602,
      "grad_norm": 0.21481861174106598,
      "learning_rate": 0.00019528781793842035,
      "loss": 0.1875,
      "step": 180
    },
    {
      "epoch": 0.07630522088353414,
      "grad_norm": 0.1573336273431778,
      "learning_rate": 0.00019502008032128517,
      "loss": 0.1953,
      "step": 190
    },
    {
      "epoch": 0.08032128514056225,
      "grad_norm": 0.18404266238212585,
      "learning_rate": 0.00019475234270414994,
      "loss": 0.1469,
      "step": 200
    },
    {
      "epoch": 0.08433734939759036,
      "grad_norm": 0.2838520109653473,
      "learning_rate": 0.00019448460508701473,
      "loss": 0.18,
      "step": 210
    },
    {
      "epoch": 0.08835341365461848,
      "grad_norm": 0.14787264168262482,
      "learning_rate": 0.00019421686746987952,
      "loss": 0.2288,
      "step": 220
    },
    {
      "epoch": 0.09236947791164658,
      "grad_norm": 0.2074081003665924,
      "learning_rate": 0.00019394912985274432,
      "loss": 0.1606,
      "step": 230
    },
    {
      "epoch": 0.0963855421686747,
      "grad_norm": 0.19937831163406372,
      "learning_rate": 0.0001936813922356091,
      "loss": 0.2306,
      "step": 240
    },
    {
      "epoch": 0.10040160642570281,
      "grad_norm": 0.16007427871227264,
      "learning_rate": 0.0001934136546184739,
      "loss": 0.1774,
      "step": 250
    },
    {
      "epoch": 0.10441767068273092,
      "grad_norm": 0.16488371789455414,
      "learning_rate": 0.0001931459170013387,
      "loss": 0.1645,
      "step": 260
    },
    {
      "epoch": 0.10843373493975904,
      "grad_norm": 0.14890284836292267,
      "learning_rate": 0.0001928781793842035,
      "loss": 0.1331,
      "step": 270
    },
    {
      "epoch": 0.11244979919678715,
      "grad_norm": 0.21429581940174103,
      "learning_rate": 0.00019261044176706828,
      "loss": 0.1817,
      "step": 280
    },
    {
      "epoch": 0.11646586345381527,
      "grad_norm": 0.1565224975347519,
      "learning_rate": 0.00019234270414993307,
      "loss": 0.1747,
      "step": 290
    },
    {
      "epoch": 0.12048192771084337,
      "grad_norm": 0.15008574724197388,
      "learning_rate": 0.00019207496653279787,
      "loss": 0.1685,
      "step": 300
    },
    {
      "epoch": 0.12449799196787148,
      "grad_norm": 0.1880067139863968,
      "learning_rate": 0.00019180722891566266,
      "loss": 0.1861,
      "step": 310
    },
    {
      "epoch": 0.1285140562248996,
      "grad_norm": 0.16637803614139557,
      "learning_rate": 0.00019153949129852745,
      "loss": 0.1216,
      "step": 320
    },
    {
      "epoch": 0.13253012048192772,
      "grad_norm": 0.15783917903900146,
      "learning_rate": 0.00019127175368139224,
      "loss": 0.1684,
      "step": 330
    },
    {
      "epoch": 0.13654618473895583,
      "grad_norm": 0.255624920129776,
      "learning_rate": 0.00019100401606425704,
      "loss": 0.1565,
      "step": 340
    },
    {
      "epoch": 0.14056224899598393,
      "grad_norm": 0.153984934091568,
      "learning_rate": 0.00019073627844712183,
      "loss": 0.1538,
      "step": 350
    },
    {
      "epoch": 0.14457831325301204,
      "grad_norm": 0.10858198255300522,
      "learning_rate": 0.00019046854082998662,
      "loss": 0.1382,
      "step": 360
    },
    {
      "epoch": 0.14859437751004015,
      "grad_norm": 0.18202553689479828,
      "learning_rate": 0.00019020080321285142,
      "loss": 0.167,
      "step": 370
    },
    {
      "epoch": 0.15261044176706828,
      "grad_norm": 0.13070832192897797,
      "learning_rate": 0.0001899330655957162,
      "loss": 0.1608,
      "step": 380
    },
    {
      "epoch": 0.1566265060240964,
      "grad_norm": 0.14221175014972687,
      "learning_rate": 0.000189665327978581,
      "loss": 0.1899,
      "step": 390
    },
    {
      "epoch": 0.1606425702811245,
      "grad_norm": 0.2027258425951004,
      "learning_rate": 0.0001893975903614458,
      "loss": 0.2041,
      "step": 400
    },
    {
      "epoch": 0.1646586345381526,
      "grad_norm": 0.1824778914451599,
      "learning_rate": 0.0001891298527443106,
      "loss": 0.1227,
      "step": 410
    },
    {
      "epoch": 0.1686746987951807,
      "grad_norm": 0.1593070775270462,
      "learning_rate": 0.00018886211512717538,
      "loss": 0.1511,
      "step": 420
    },
    {
      "epoch": 0.17269076305220885,
      "grad_norm": 0.13236947357654572,
      "learning_rate": 0.00018859437751004017,
      "loss": 0.1617,
      "step": 430
    },
    {
      "epoch": 0.17670682730923695,
      "grad_norm": 0.1416393667459488,
      "learning_rate": 0.00018832663989290497,
      "loss": 0.1855,
      "step": 440
    },
    {
      "epoch": 0.18072289156626506,
      "grad_norm": 0.13801097869873047,
      "learning_rate": 0.00018805890227576973,
      "loss": 0.1087,
      "step": 450
    },
    {
      "epoch": 0.18473895582329317,
      "grad_norm": 0.15512816607952118,
      "learning_rate": 0.00018779116465863455,
      "loss": 0.1441,
      "step": 460
    },
    {
      "epoch": 0.18875502008032127,
      "grad_norm": 0.13650621473789215,
      "learning_rate": 0.00018752342704149934,
      "loss": 0.1456,
      "step": 470
    },
    {
      "epoch": 0.1927710843373494,
      "grad_norm": 0.16228686273097992,
      "learning_rate": 0.00018725568942436414,
      "loss": 0.1169,
      "step": 480
    },
    {
      "epoch": 0.19678714859437751,
      "grad_norm": 0.17383013665676117,
      "learning_rate": 0.00018698795180722893,
      "loss": 0.1697,
      "step": 490
    },
    {
      "epoch": 0.20080321285140562,
      "grad_norm": 0.12401378899812698,
      "learning_rate": 0.00018672021419009372,
      "loss": 0.188,
      "step": 500
    },
    {
      "epoch": 0.20481927710843373,
      "grad_norm": 0.09790448099374771,
      "learning_rate": 0.0001864524765729585,
      "loss": 0.1653,
      "step": 510
    },
    {
      "epoch": 0.20883534136546184,
      "grad_norm": 0.12724439799785614,
      "learning_rate": 0.0001861847389558233,
      "loss": 0.1454,
      "step": 520
    },
    {
      "epoch": 0.21285140562248997,
      "grad_norm": 0.20847299695014954,
      "learning_rate": 0.0001859170013386881,
      "loss": 0.1238,
      "step": 530
    },
    {
      "epoch": 0.21686746987951808,
      "grad_norm": 0.20052847266197205,
      "learning_rate": 0.0001856492637215529,
      "loss": 0.1501,
      "step": 540
    },
    {
      "epoch": 0.22088353413654618,
      "grad_norm": 0.15275733172893524,
      "learning_rate": 0.0001853815261044177,
      "loss": 0.1751,
      "step": 550
    },
    {
      "epoch": 0.2248995983935743,
      "grad_norm": 0.14660954475402832,
      "learning_rate": 0.00018511378848728248,
      "loss": 0.14,
      "step": 560
    },
    {
      "epoch": 0.2289156626506024,
      "grad_norm": 0.21540865302085876,
      "learning_rate": 0.00018484605087014725,
      "loss": 0.1439,
      "step": 570
    },
    {
      "epoch": 0.23293172690763053,
      "grad_norm": 0.1386047750711441,
      "learning_rate": 0.00018457831325301204,
      "loss": 0.1192,
      "step": 580
    },
    {
      "epoch": 0.23694779116465864,
      "grad_norm": 0.16499263048171997,
      "learning_rate": 0.00018431057563587686,
      "loss": 0.1817,
      "step": 590
    },
    {
      "epoch": 0.24096385542168675,
      "grad_norm": 0.11850390583276749,
      "learning_rate": 0.00018404283801874165,
      "loss": 0.1207,
      "step": 600
    },
    {
      "epoch": 0.24497991967871485,
      "grad_norm": 0.11292551457881927,
      "learning_rate": 0.00018377510040160644,
      "loss": 0.1515,
      "step": 610
    },
    {
      "epoch": 0.24899598393574296,
      "grad_norm": 0.13298781216144562,
      "learning_rate": 0.00018350736278447124,
      "loss": 0.1459,
      "step": 620
    },
    {
      "epoch": 0.25301204819277107,
      "grad_norm": 0.13340459764003754,
      "learning_rate": 0.000183239625167336,
      "loss": 0.1381,
      "step": 630
    },
    {
      "epoch": 0.2570281124497992,
      "grad_norm": 0.11658608913421631,
      "learning_rate": 0.0001829718875502008,
      "loss": 0.1271,
      "step": 640
    },
    {
      "epoch": 0.26104417670682734,
      "grad_norm": 0.15069490671157837,
      "learning_rate": 0.0001827041499330656,
      "loss": 0.1502,
      "step": 650
    },
    {
      "epoch": 0.26506024096385544,
      "grad_norm": 0.19711509346961975,
      "learning_rate": 0.0001824364123159304,
      "loss": 0.1798,
      "step": 660
    },
    {
      "epoch": 0.26907630522088355,
      "grad_norm": 0.1677546352148056,
      "learning_rate": 0.0001821686746987952,
      "loss": 0.1388,
      "step": 670
    },
    {
      "epoch": 0.27309236947791166,
      "grad_norm": 0.26692306995391846,
      "learning_rate": 0.00018190093708166,
      "loss": 0.2311,
      "step": 680
    },
    {
      "epoch": 0.27710843373493976,
      "grad_norm": 0.15271097421646118,
      "learning_rate": 0.00018163319946452479,
      "loss": 0.1428,
      "step": 690
    },
    {
      "epoch": 0.28112449799196787,
      "grad_norm": 0.11359474062919617,
      "learning_rate": 0.00018136546184738955,
      "loss": 0.1498,
      "step": 700
    },
    {
      "epoch": 0.285140562248996,
      "grad_norm": 0.12054453045129776,
      "learning_rate": 0.00018109772423025434,
      "loss": 0.18,
      "step": 710
    },
    {
      "epoch": 0.2891566265060241,
      "grad_norm": 0.22329112887382507,
      "learning_rate": 0.00018082998661311916,
      "loss": 0.1981,
      "step": 720
    },
    {
      "epoch": 0.2931726907630522,
      "grad_norm": 0.1644359976053238,
      "learning_rate": 0.00018056224899598396,
      "loss": 0.1631,
      "step": 730
    },
    {
      "epoch": 0.2971887550200803,
      "grad_norm": 0.27183371782302856,
      "learning_rate": 0.00018029451137884875,
      "loss": 0.1578,
      "step": 740
    },
    {
      "epoch": 0.30120481927710846,
      "grad_norm": 0.1351560801267624,
      "learning_rate": 0.00018002677376171354,
      "loss": 0.1794,
      "step": 750
    },
    {
      "epoch": 0.30522088353413657,
      "grad_norm": 0.1451903134584427,
      "learning_rate": 0.0001797590361445783,
      "loss": 0.1745,
      "step": 760
    },
    {
      "epoch": 0.3092369477911647,
      "grad_norm": 0.15380533039569855,
      "learning_rate": 0.0001794912985274431,
      "loss": 0.1415,
      "step": 770
    },
    {
      "epoch": 0.3132530120481928,
      "grad_norm": 0.17916899919509888,
      "learning_rate": 0.0001792235609103079,
      "loss": 0.142,
      "step": 780
    },
    {
      "epoch": 0.3172690763052209,
      "grad_norm": 0.11935751885175705,
      "learning_rate": 0.00017895582329317271,
      "loss": 0.1474,
      "step": 790
    },
    {
      "epoch": 0.321285140562249,
      "grad_norm": 0.14896373450756073,
      "learning_rate": 0.0001786880856760375,
      "loss": 0.1409,
      "step": 800
    },
    {
      "epoch": 0.3253012048192771,
      "grad_norm": 0.18507492542266846,
      "learning_rate": 0.0001784203480589023,
      "loss": 0.1332,
      "step": 810
    },
    {
      "epoch": 0.3293172690763052,
      "grad_norm": 0.1385105401277542,
      "learning_rate": 0.00017815261044176707,
      "loss": 0.163,
      "step": 820
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.11274965852499008,
      "learning_rate": 0.00017788487282463186,
      "loss": 0.1533,
      "step": 830
    },
    {
      "epoch": 0.3373493975903614,
      "grad_norm": 0.11774027347564697,
      "learning_rate": 0.00017761713520749665,
      "loss": 0.2024,
      "step": 840
    },
    {
      "epoch": 0.3413654618473896,
      "grad_norm": 0.19043783843517303,
      "learning_rate": 0.00017734939759036144,
      "loss": 0.1457,
      "step": 850
    },
    {
      "epoch": 0.3453815261044177,
      "grad_norm": 0.14338000118732452,
      "learning_rate": 0.00017708165997322626,
      "loss": 0.1536,
      "step": 860
    },
    {
      "epoch": 0.3493975903614458,
      "grad_norm": 0.1685604453086853,
      "learning_rate": 0.00017681392235609106,
      "loss": 0.152,
      "step": 870
    },
    {
      "epoch": 0.3534136546184739,
      "grad_norm": 0.12192444503307343,
      "learning_rate": 0.00017654618473895582,
      "loss": 0.1535,
      "step": 880
    },
    {
      "epoch": 0.357429718875502,
      "grad_norm": 0.15489143133163452,
      "learning_rate": 0.00017627844712182062,
      "loss": 0.1623,
      "step": 890
    },
    {
      "epoch": 0.3614457831325301,
      "grad_norm": 0.1711120307445526,
      "learning_rate": 0.0001760107095046854,
      "loss": 0.1542,
      "step": 900
    },
    {
      "epoch": 0.3654618473895582,
      "grad_norm": 0.1635996699333191,
      "learning_rate": 0.0001757429718875502,
      "loss": 0.1716,
      "step": 910
    },
    {
      "epoch": 0.36947791164658633,
      "grad_norm": 0.22671273350715637,
      "learning_rate": 0.00017547523427041502,
      "loss": 0.1587,
      "step": 920
    },
    {
      "epoch": 0.37349397590361444,
      "grad_norm": 0.14747101068496704,
      "learning_rate": 0.00017520749665327981,
      "loss": 0.1653,
      "step": 930
    },
    {
      "epoch": 0.37751004016064255,
      "grad_norm": 0.24242594838142395,
      "learning_rate": 0.00017493975903614458,
      "loss": 0.1779,
      "step": 940
    },
    {
      "epoch": 0.3815261044176707,
      "grad_norm": 0.13269324600696564,
      "learning_rate": 0.00017467202141900937,
      "loss": 0.1491,
      "step": 950
    },
    {
      "epoch": 0.3855421686746988,
      "grad_norm": 0.13110795617103577,
      "learning_rate": 0.00017440428380187416,
      "loss": 0.1438,
      "step": 960
    },
    {
      "epoch": 0.3895582329317269,
      "grad_norm": 0.2016555815935135,
      "learning_rate": 0.00017413654618473896,
      "loss": 0.1632,
      "step": 970
    },
    {
      "epoch": 0.39357429718875503,
      "grad_norm": 0.15869878232479095,
      "learning_rate": 0.00017386880856760375,
      "loss": 0.1666,
      "step": 980
    },
    {
      "epoch": 0.39759036144578314,
      "grad_norm": 0.2236054688692093,
      "learning_rate": 0.00017360107095046857,
      "loss": 0.1843,
      "step": 990
    },
    {
      "epoch": 0.40160642570281124,
      "grad_norm": 0.178426593542099,
      "learning_rate": 0.00017333333333333334,
      "loss": 0.1504,
      "step": 1000
    },
    {
      "epoch": 0.40562248995983935,
      "grad_norm": 0.17807555198669434,
      "learning_rate": 0.00017306559571619813,
      "loss": 0.1315,
      "step": 1010
    },
    {
      "epoch": 0.40963855421686746,
      "grad_norm": 0.13551288843154907,
      "learning_rate": 0.00017279785809906292,
      "loss": 0.1715,
      "step": 1020
    },
    {
      "epoch": 0.41365461847389556,
      "grad_norm": 0.1560937464237213,
      "learning_rate": 0.00017253012048192771,
      "loss": 0.1578,
      "step": 1030
    },
    {
      "epoch": 0.41767068273092367,
      "grad_norm": 0.16840717196464539,
      "learning_rate": 0.0001722623828647925,
      "loss": 0.1761,
      "step": 1040
    },
    {
      "epoch": 0.42168674698795183,
      "grad_norm": 0.138344407081604,
      "learning_rate": 0.0001719946452476573,
      "loss": 0.1724,
      "step": 1050
    },
    {
      "epoch": 0.42570281124497994,
      "grad_norm": 0.1511981338262558,
      "learning_rate": 0.0001717269076305221,
      "loss": 0.159,
      "step": 1060
    },
    {
      "epoch": 0.42971887550200805,
      "grad_norm": 0.15057812631130219,
      "learning_rate": 0.00017145917001338689,
      "loss": 0.1332,
      "step": 1070
    },
    {
      "epoch": 0.43373493975903615,
      "grad_norm": 0.15636631846427917,
      "learning_rate": 0.00017119143239625168,
      "loss": 0.1749,
      "step": 1080
    },
    {
      "epoch": 0.43775100401606426,
      "grad_norm": 0.09414291381835938,
      "learning_rate": 0.00017092369477911647,
      "loss": 0.1636,
      "step": 1090
    },
    {
      "epoch": 0.44176706827309237,
      "grad_norm": 0.13017037510871887,
      "learning_rate": 0.00017065595716198126,
      "loss": 0.1177,
      "step": 1100
    },
    {
      "epoch": 0.4457831325301205,
      "grad_norm": 0.1808108240365982,
      "learning_rate": 0.00017038821954484606,
      "loss": 0.1479,
      "step": 1110
    },
    {
      "epoch": 0.4497991967871486,
      "grad_norm": 0.17855042219161987,
      "learning_rate": 0.00017012048192771085,
      "loss": 0.1482,
      "step": 1120
    },
    {
      "epoch": 0.4538152610441767,
      "grad_norm": 0.1355665773153305,
      "learning_rate": 0.00016985274431057564,
      "loss": 0.1181,
      "step": 1130
    },
    {
      "epoch": 0.4578313253012048,
      "grad_norm": 0.19207492470741272,
      "learning_rate": 0.00016958500669344044,
      "loss": 0.1501,
      "step": 1140
    },
    {
      "epoch": 0.46184738955823296,
      "grad_norm": 0.13961511850357056,
      "learning_rate": 0.00016931726907630523,
      "loss": 0.1497,
      "step": 1150
    },
    {
      "epoch": 0.46586345381526106,
      "grad_norm": 0.1515282839536667,
      "learning_rate": 0.00016904953145917002,
      "loss": 0.1629,
      "step": 1160
    },
    {
      "epoch": 0.46987951807228917,
      "grad_norm": 0.19519945979118347,
      "learning_rate": 0.00016878179384203481,
      "loss": 0.1312,
      "step": 1170
    },
    {
      "epoch": 0.4738955823293173,
      "grad_norm": 0.1350020170211792,
      "learning_rate": 0.0001685140562248996,
      "loss": 0.1781,
      "step": 1180
    },
    {
      "epoch": 0.4779116465863454,
      "grad_norm": 0.13612115383148193,
      "learning_rate": 0.0001682463186077644,
      "loss": 0.1839,
      "step": 1190
    },
    {
      "epoch": 0.4819277108433735,
      "grad_norm": 0.12584763765335083,
      "learning_rate": 0.0001679785809906292,
      "loss": 0.1415,
      "step": 1200
    },
    {
      "epoch": 0.4859437751004016,
      "grad_norm": 0.16870222985744476,
      "learning_rate": 0.00016771084337349399,
      "loss": 0.1898,
      "step": 1210
    },
    {
      "epoch": 0.4899598393574297,
      "grad_norm": 0.19976180791854858,
      "learning_rate": 0.00016744310575635878,
      "loss": 0.1767,
      "step": 1220
    },
    {
      "epoch": 0.4939759036144578,
      "grad_norm": 0.10171814262866974,
      "learning_rate": 0.00016717536813922357,
      "loss": 0.1563,
      "step": 1230
    },
    {
      "epoch": 0.4979919678714859,
      "grad_norm": 0.16575543582439423,
      "learning_rate": 0.00016690763052208836,
      "loss": 0.1734,
      "step": 1240
    },
    {
      "epoch": 0.5020080321285141,
      "grad_norm": 0.1446106880903244,
      "learning_rate": 0.00016663989290495316,
      "loss": 0.122,
      "step": 1250
    },
    {
      "epoch": 0.5060240963855421,
      "grad_norm": 0.18652759492397308,
      "learning_rate": 0.00016637215528781795,
      "loss": 0.1729,
      "step": 1260
    },
    {
      "epoch": 0.5100401606425703,
      "grad_norm": 0.14889474213123322,
      "learning_rate": 0.00016610441767068274,
      "loss": 0.1719,
      "step": 1270
    },
    {
      "epoch": 0.5140562248995983,
      "grad_norm": 0.13867893815040588,
      "learning_rate": 0.00016583668005354754,
      "loss": 0.1468,
      "step": 1280
    },
    {
      "epoch": 0.5180722891566265,
      "grad_norm": 0.12560537457466125,
      "learning_rate": 0.00016556894243641233,
      "loss": 0.1503,
      "step": 1290
    },
    {
      "epoch": 0.5220883534136547,
      "grad_norm": 0.1710757166147232,
      "learning_rate": 0.00016530120481927712,
      "loss": 0.1649,
      "step": 1300
    },
    {
      "epoch": 0.5261044176706827,
      "grad_norm": 0.126078799366951,
      "learning_rate": 0.0001650334672021419,
      "loss": 0.1231,
      "step": 1310
    },
    {
      "epoch": 0.5301204819277109,
      "grad_norm": 0.18684734404087067,
      "learning_rate": 0.0001647657295850067,
      "loss": 0.1381,
      "step": 1320
    },
    {
      "epoch": 0.5341365461847389,
      "grad_norm": 0.14177273213863373,
      "learning_rate": 0.0001644979919678715,
      "loss": 0.1639,
      "step": 1330
    },
    {
      "epoch": 0.5381526104417671,
      "grad_norm": 0.13033893704414368,
      "learning_rate": 0.0001642302543507363,
      "loss": 0.1286,
      "step": 1340
    },
    {
      "epoch": 0.5421686746987951,
      "grad_norm": 0.2068467140197754,
      "learning_rate": 0.00016396251673360108,
      "loss": 0.1817,
      "step": 1350
    },
    {
      "epoch": 0.5461847389558233,
      "grad_norm": 0.16568896174430847,
      "learning_rate": 0.00016369477911646588,
      "loss": 0.1662,
      "step": 1360
    },
    {
      "epoch": 0.5502008032128514,
      "grad_norm": 0.1512761116027832,
      "learning_rate": 0.00016342704149933067,
      "loss": 0.1404,
      "step": 1370
    },
    {
      "epoch": 0.5542168674698795,
      "grad_norm": 0.17478536069393158,
      "learning_rate": 0.00016315930388219544,
      "loss": 0.133,
      "step": 1380
    },
    {
      "epoch": 0.5582329317269076,
      "grad_norm": 0.16219426691532135,
      "learning_rate": 0.00016289156626506026,
      "loss": 0.1784,
      "step": 1390
    },
    {
      "epoch": 0.5622489959839357,
      "grad_norm": 0.2449439913034439,
      "learning_rate": 0.00016262382864792505,
      "loss": 0.1477,
      "step": 1400
    },
    {
      "epoch": 0.5662650602409639,
      "grad_norm": 0.16494803130626678,
      "learning_rate": 0.00016235609103078984,
      "loss": 0.1566,
      "step": 1410
    },
    {
      "epoch": 0.570281124497992,
      "grad_norm": 0.1676202416419983,
      "learning_rate": 0.00016208835341365463,
      "loss": 0.1217,
      "step": 1420
    },
    {
      "epoch": 0.5742971887550201,
      "grad_norm": 0.15852363407611847,
      "learning_rate": 0.00016182061579651943,
      "loss": 0.1824,
      "step": 1430
    },
    {
      "epoch": 0.5783132530120482,
      "grad_norm": 0.14533358812332153,
      "learning_rate": 0.0001615528781793842,
      "loss": 0.1206,
      "step": 1440
    },
    {
      "epoch": 0.5823293172690763,
      "grad_norm": 0.17694075405597687,
      "learning_rate": 0.00016128514056224899,
      "loss": 0.1744,
      "step": 1450
    },
    {
      "epoch": 0.5863453815261044,
      "grad_norm": 0.20675936341285706,
      "learning_rate": 0.0001610174029451138,
      "loss": 0.183,
      "step": 1460
    },
    {
      "epoch": 0.5903614457831325,
      "grad_norm": 0.1765468716621399,
      "learning_rate": 0.0001607496653279786,
      "loss": 0.1845,
      "step": 1470
    },
    {
      "epoch": 0.5943775100401606,
      "grad_norm": 0.14145928621292114,
      "learning_rate": 0.0001604819277108434,
      "loss": 0.116,
      "step": 1480
    },
    {
      "epoch": 0.5983935742971888,
      "grad_norm": 0.1384068876504898,
      "learning_rate": 0.00016021419009370818,
      "loss": 0.1217,
      "step": 1490
    },
    {
      "epoch": 0.6024096385542169,
      "grad_norm": 0.1646496206521988,
      "learning_rate": 0.00015994645247657295,
      "loss": 0.1713,
      "step": 1500
    },
    {
      "epoch": 0.606425702811245,
      "grad_norm": 0.2034090906381607,
      "learning_rate": 0.00015967871485943774,
      "loss": 0.1345,
      "step": 1510
    },
    {
      "epoch": 0.6104417670682731,
      "grad_norm": 0.18858778476715088,
      "learning_rate": 0.00015941097724230256,
      "loss": 0.1637,
      "step": 1520
    },
    {
      "epoch": 0.6144578313253012,
      "grad_norm": 0.14503659307956696,
      "learning_rate": 0.00015914323962516736,
      "loss": 0.1307,
      "step": 1530
    },
    {
      "epoch": 0.6184738955823293,
      "grad_norm": 0.12711533904075623,
      "learning_rate": 0.00015887550200803215,
      "loss": 0.1089,
      "step": 1540
    },
    {
      "epoch": 0.6224899598393574,
      "grad_norm": 0.1298924833536148,
      "learning_rate": 0.00015860776439089694,
      "loss": 0.1674,
      "step": 1550
    },
    {
      "epoch": 0.6265060240963856,
      "grad_norm": 0.14181561768054962,
      "learning_rate": 0.0001583400267737617,
      "loss": 0.1219,
      "step": 1560
    },
    {
      "epoch": 0.6305220883534136,
      "grad_norm": 0.13411593437194824,
      "learning_rate": 0.0001580722891566265,
      "loss": 0.1702,
      "step": 1570
    },
    {
      "epoch": 0.6345381526104418,
      "grad_norm": 0.19297632575035095,
      "learning_rate": 0.0001578045515394913,
      "loss": 0.1305,
      "step": 1580
    },
    {
      "epoch": 0.6385542168674698,
      "grad_norm": 0.16957364976406097,
      "learning_rate": 0.0001575368139223561,
      "loss": 0.1523,
      "step": 1590
    },
    {
      "epoch": 0.642570281124498,
      "grad_norm": 0.15720896422863007,
      "learning_rate": 0.0001572690763052209,
      "loss": 0.2011,
      "step": 1600
    },
    {
      "epoch": 0.6465863453815262,
      "grad_norm": 0.12194468826055527,
      "learning_rate": 0.0001570013386880857,
      "loss": 0.1165,
      "step": 1610
    },
    {
      "epoch": 0.6506024096385542,
      "grad_norm": 0.1913793683052063,
      "learning_rate": 0.00015673360107095046,
      "loss": 0.1456,
      "step": 1620
    },
    {
      "epoch": 0.6546184738955824,
      "grad_norm": 0.1666683852672577,
      "learning_rate": 0.00015646586345381526,
      "loss": 0.1367,
      "step": 1630
    },
    {
      "epoch": 0.6586345381526104,
      "grad_norm": 0.1570734977722168,
      "learning_rate": 0.00015619812583668005,
      "loss": 0.138,
      "step": 1640
    },
    {
      "epoch": 0.6626506024096386,
      "grad_norm": 0.13522537052631378,
      "learning_rate": 0.00015593038821954484,
      "loss": 0.1418,
      "step": 1650
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.11988849937915802,
      "learning_rate": 0.00015566265060240966,
      "loss": 0.1508,
      "step": 1660
    },
    {
      "epoch": 0.6706827309236948,
      "grad_norm": 0.1615239381790161,
      "learning_rate": 0.00015539491298527445,
      "loss": 0.1391,
      "step": 1670
    },
    {
      "epoch": 0.6746987951807228,
      "grad_norm": 0.14989010989665985,
      "learning_rate": 0.00015512717536813922,
      "loss": 0.1495,
      "step": 1680
    },
    {
      "epoch": 0.678714859437751,
      "grad_norm": 0.1414366364479065,
      "learning_rate": 0.000154859437751004,
      "loss": 0.1504,
      "step": 1690
    },
    {
      "epoch": 0.6827309236947792,
      "grad_norm": 0.13701817393302917,
      "learning_rate": 0.0001545917001338688,
      "loss": 0.1503,
      "step": 1700
    },
    {
      "epoch": 0.6867469879518072,
      "grad_norm": 0.17706459760665894,
      "learning_rate": 0.0001543239625167336,
      "loss": 0.1664,
      "step": 1710
    },
    {
      "epoch": 0.6907630522088354,
      "grad_norm": 0.1573748141527176,
      "learning_rate": 0.00015405622489959842,
      "loss": 0.1322,
      "step": 1720
    },
    {
      "epoch": 0.6947791164658634,
      "grad_norm": 0.18719801306724548,
      "learning_rate": 0.0001537884872824632,
      "loss": 0.1762,
      "step": 1730
    },
    {
      "epoch": 0.6987951807228916,
      "grad_norm": 0.1446426659822464,
      "learning_rate": 0.00015352074966532798,
      "loss": 0.1365,
      "step": 1740
    },
    {
      "epoch": 0.7028112449799196,
      "grad_norm": 0.14664840698242188,
      "learning_rate": 0.00015325301204819277,
      "loss": 0.1422,
      "step": 1750
    },
    {
      "epoch": 0.7068273092369478,
      "grad_norm": 0.14992567896842957,
      "learning_rate": 0.00015298527443105756,
      "loss": 0.1377,
      "step": 1760
    },
    {
      "epoch": 0.7108433734939759,
      "grad_norm": 0.20165078341960907,
      "learning_rate": 0.00015271753681392236,
      "loss": 0.1707,
      "step": 1770
    },
    {
      "epoch": 0.714859437751004,
      "grad_norm": 0.18316949903964996,
      "learning_rate": 0.00015244979919678715,
      "loss": 0.1636,
      "step": 1780
    },
    {
      "epoch": 0.7188755020080321,
      "grad_norm": 0.11523500084877014,
      "learning_rate": 0.00015218206157965197,
      "loss": 0.1467,
      "step": 1790
    },
    {
      "epoch": 0.7228915662650602,
      "grad_norm": 0.1483432948589325,
      "learning_rate": 0.00015191432396251673,
      "loss": 0.1797,
      "step": 1800
    },
    {
      "epoch": 0.7269076305220884,
      "grad_norm": 0.1268533617258072,
      "learning_rate": 0.00015164658634538153,
      "loss": 0.1509,
      "step": 1810
    },
    {
      "epoch": 0.7309236947791165,
      "grad_norm": 0.12657074630260468,
      "learning_rate": 0.00015137884872824632,
      "loss": 0.1242,
      "step": 1820
    },
    {
      "epoch": 0.7349397590361446,
      "grad_norm": 0.20312917232513428,
      "learning_rate": 0.0001511111111111111,
      "loss": 0.1585,
      "step": 1830
    },
    {
      "epoch": 0.7389558232931727,
      "grad_norm": 0.1361571103334427,
      "learning_rate": 0.0001508433734939759,
      "loss": 0.175,
      "step": 1840
    },
    {
      "epoch": 0.7429718875502008,
      "grad_norm": 0.16797319054603577,
      "learning_rate": 0.0001505756358768407,
      "loss": 0.1171,
      "step": 1850
    },
    {
      "epoch": 0.7469879518072289,
      "grad_norm": 0.22230452299118042,
      "learning_rate": 0.0001503078982597055,
      "loss": 0.1407,
      "step": 1860
    },
    {
      "epoch": 0.751004016064257,
      "grad_norm": 0.15182189643383026,
      "learning_rate": 0.00015004016064257028,
      "loss": 0.1229,
      "step": 1870
    },
    {
      "epoch": 0.7550200803212851,
      "grad_norm": 0.13179554045200348,
      "learning_rate": 0.00014977242302543508,
      "loss": 0.1799,
      "step": 1880
    },
    {
      "epoch": 0.7590361445783133,
      "grad_norm": 0.22870773077011108,
      "learning_rate": 0.00014950468540829987,
      "loss": 0.159,
      "step": 1890
    },
    {
      "epoch": 0.7630522088353414,
      "grad_norm": 0.13298849761486053,
      "learning_rate": 0.00014923694779116466,
      "loss": 0.1523,
      "step": 1900
    },
    {
      "epoch": 0.7670682730923695,
      "grad_norm": 0.13293488323688507,
      "learning_rate": 0.00014896921017402946,
      "loss": 0.14,
      "step": 1910
    },
    {
      "epoch": 0.7710843373493976,
      "grad_norm": 0.12157325446605682,
      "learning_rate": 0.00014870147255689425,
      "loss": 0.152,
      "step": 1920
    },
    {
      "epoch": 0.7751004016064257,
      "grad_norm": 0.2803887128829956,
      "learning_rate": 0.00014843373493975904,
      "loss": 0.1765,
      "step": 1930
    },
    {
      "epoch": 0.7791164658634538,
      "grad_norm": 0.27045729756355286,
      "learning_rate": 0.00014816599732262383,
      "loss": 0.1545,
      "step": 1940
    },
    {
      "epoch": 0.7831325301204819,
      "grad_norm": 0.174331933259964,
      "learning_rate": 0.00014789825970548863,
      "loss": 0.1636,
      "step": 1950
    },
    {
      "epoch": 0.7871485943775101,
      "grad_norm": 0.14737986028194427,
      "learning_rate": 0.00014763052208835342,
      "loss": 0.0956,
      "step": 1960
    },
    {
      "epoch": 0.7911646586345381,
      "grad_norm": 0.157323956489563,
      "learning_rate": 0.0001473627844712182,
      "loss": 0.1434,
      "step": 1970
    },
    {
      "epoch": 0.7951807228915663,
      "grad_norm": 0.23479250073432922,
      "learning_rate": 0.000147095046854083,
      "loss": 0.1215,
      "step": 1980
    },
    {
      "epoch": 0.7991967871485943,
      "grad_norm": 0.17147290706634521,
      "learning_rate": 0.0001468273092369478,
      "loss": 0.1272,
      "step": 1990
    },
    {
      "epoch": 0.8032128514056225,
      "grad_norm": 0.146162748336792,
      "learning_rate": 0.0001465595716198126,
      "loss": 0.17,
      "step": 2000
    },
    {
      "epoch": 0.8072289156626506,
      "grad_norm": 0.15895162522792816,
      "learning_rate": 0.00014629183400267738,
      "loss": 0.1342,
      "step": 2010
    },
    {
      "epoch": 0.8112449799196787,
      "grad_norm": 0.1418740600347519,
      "learning_rate": 0.00014602409638554218,
      "loss": 0.1433,
      "step": 2020
    },
    {
      "epoch": 0.8152610441767069,
      "grad_norm": 0.13832919299602509,
      "learning_rate": 0.00014575635876840697,
      "loss": 0.1251,
      "step": 2030
    },
    {
      "epoch": 0.8192771084337349,
      "grad_norm": 0.1706903725862503,
      "learning_rate": 0.00014548862115127176,
      "loss": 0.1435,
      "step": 2040
    },
    {
      "epoch": 0.8232931726907631,
      "grad_norm": 0.1428888589143753,
      "learning_rate": 0.00014522088353413655,
      "loss": 0.1476,
      "step": 2050
    },
    {
      "epoch": 0.8273092369477911,
      "grad_norm": 0.16879449784755707,
      "learning_rate": 0.00014495314591700135,
      "loss": 0.181,
      "step": 2060
    },
    {
      "epoch": 0.8313253012048193,
      "grad_norm": 0.14744998514652252,
      "learning_rate": 0.00014468540829986614,
      "loss": 0.1475,
      "step": 2070
    },
    {
      "epoch": 0.8353413654618473,
      "grad_norm": 0.15174032747745514,
      "learning_rate": 0.00014441767068273093,
      "loss": 0.1922,
      "step": 2080
    },
    {
      "epoch": 0.8393574297188755,
      "grad_norm": 0.2093653529882431,
      "learning_rate": 0.00014414993306559573,
      "loss": 0.148,
      "step": 2090
    },
    {
      "epoch": 0.8433734939759037,
      "grad_norm": 0.1728537678718567,
      "learning_rate": 0.00014388219544846052,
      "loss": 0.1377,
      "step": 2100
    },
    {
      "epoch": 0.8473895582329317,
      "grad_norm": 0.14544805884361267,
      "learning_rate": 0.0001436144578313253,
      "loss": 0.1633,
      "step": 2110
    },
    {
      "epoch": 0.8514056224899599,
      "grad_norm": 0.22361646592617035,
      "learning_rate": 0.0001433467202141901,
      "loss": 0.1344,
      "step": 2120
    },
    {
      "epoch": 0.8554216867469879,
      "grad_norm": 0.1855863779783249,
      "learning_rate": 0.0001430789825970549,
      "loss": 0.107,
      "step": 2130
    },
    {
      "epoch": 0.8594377510040161,
      "grad_norm": 0.140947163105011,
      "learning_rate": 0.0001428112449799197,
      "loss": 0.1452,
      "step": 2140
    },
    {
      "epoch": 0.8634538152610441,
      "grad_norm": 0.13547131419181824,
      "learning_rate": 0.00014254350736278448,
      "loss": 0.1057,
      "step": 2150
    },
    {
      "epoch": 0.8674698795180723,
      "grad_norm": 0.15266266465187073,
      "learning_rate": 0.00014227576974564928,
      "loss": 0.1291,
      "step": 2160
    },
    {
      "epoch": 0.8714859437751004,
      "grad_norm": 0.14990490674972534,
      "learning_rate": 0.00014200803212851407,
      "loss": 0.1133,
      "step": 2170
    },
    {
      "epoch": 0.8755020080321285,
      "grad_norm": 0.1931552141904831,
      "learning_rate": 0.00014174029451137883,
      "loss": 0.1395,
      "step": 2180
    },
    {
      "epoch": 0.8795180722891566,
      "grad_norm": 0.20146194100379944,
      "learning_rate": 0.00014147255689424365,
      "loss": 0.2296,
      "step": 2190
    },
    {
      "epoch": 0.8835341365461847,
      "grad_norm": 0.18594665825366974,
      "learning_rate": 0.00014120481927710845,
      "loss": 0.1458,
      "step": 2200
    },
    {
      "epoch": 0.8875502008032129,
      "grad_norm": 0.16860772669315338,
      "learning_rate": 0.00014093708165997324,
      "loss": 0.1118,
      "step": 2210
    },
    {
      "epoch": 0.891566265060241,
      "grad_norm": 0.14327372610569,
      "learning_rate": 0.00014066934404283803,
      "loss": 0.1304,
      "step": 2220
    },
    {
      "epoch": 0.8955823293172691,
      "grad_norm": 0.19004997611045837,
      "learning_rate": 0.00014040160642570283,
      "loss": 0.1699,
      "step": 2230
    },
    {
      "epoch": 0.8995983935742972,
      "grad_norm": 0.15001191198825836,
      "learning_rate": 0.0001401338688085676,
      "loss": 0.119,
      "step": 2240
    },
    {
      "epoch": 0.9036144578313253,
      "grad_norm": 0.12003504484891891,
      "learning_rate": 0.00013986613119143238,
      "loss": 0.173,
      "step": 2250
    },
    {
      "epoch": 0.9076305220883534,
      "grad_norm": 0.1998170018196106,
      "learning_rate": 0.0001395983935742972,
      "loss": 0.1523,
      "step": 2260
    },
    {
      "epoch": 0.9116465863453815,
      "grad_norm": 0.12704575061798096,
      "learning_rate": 0.000139330655957162,
      "loss": 0.1245,
      "step": 2270
    },
    {
      "epoch": 0.9156626506024096,
      "grad_norm": 0.13816171884536743,
      "learning_rate": 0.0001390629183400268,
      "loss": 0.161,
      "step": 2280
    },
    {
      "epoch": 0.9196787148594378,
      "grad_norm": 0.1560342013835907,
      "learning_rate": 0.00013879518072289158,
      "loss": 0.1445,
      "step": 2290
    },
    {
      "epoch": 0.9236947791164659,
      "grad_norm": 0.1514168679714203,
      "learning_rate": 0.00013852744310575635,
      "loss": 0.1582,
      "step": 2300
    },
    {
      "epoch": 0.927710843373494,
      "grad_norm": 0.1497749239206314,
      "learning_rate": 0.00013825970548862114,
      "loss": 0.1073,
      "step": 2310
    },
    {
      "epoch": 0.9317269076305221,
      "grad_norm": 0.12062451243400574,
      "learning_rate": 0.00013799196787148596,
      "loss": 0.166,
      "step": 2320
    },
    {
      "epoch": 0.9357429718875502,
      "grad_norm": 0.1867484450340271,
      "learning_rate": 0.00013772423025435075,
      "loss": 0.1412,
      "step": 2330
    },
    {
      "epoch": 0.9397590361445783,
      "grad_norm": 0.10965792834758759,
      "learning_rate": 0.00013745649263721555,
      "loss": 0.1069,
      "step": 2340
    },
    {
      "epoch": 0.9437751004016064,
      "grad_norm": 0.15761560201644897,
      "learning_rate": 0.00013718875502008034,
      "loss": 0.1519,
      "step": 2350
    },
    {
      "epoch": 0.9477911646586346,
      "grad_norm": 0.166342094540596,
      "learning_rate": 0.0001369210174029451,
      "loss": 0.1457,
      "step": 2360
    },
    {
      "epoch": 0.9518072289156626,
      "grad_norm": 0.14710386097431183,
      "learning_rate": 0.0001366532797858099,
      "loss": 0.1206,
      "step": 2370
    },
    {
      "epoch": 0.9558232931726908,
      "grad_norm": 0.1507560908794403,
      "learning_rate": 0.0001363855421686747,
      "loss": 0.1837,
      "step": 2380
    },
    {
      "epoch": 0.9598393574297188,
      "grad_norm": 0.1197100579738617,
      "learning_rate": 0.0001361178045515395,
      "loss": 0.1258,
      "step": 2390
    },
    {
      "epoch": 0.963855421686747,
      "grad_norm": 0.18566913902759552,
      "learning_rate": 0.0001358500669344043,
      "loss": 0.1398,
      "step": 2400
    },
    {
      "epoch": 0.9678714859437751,
      "grad_norm": 0.14627191424369812,
      "learning_rate": 0.0001355823293172691,
      "loss": 0.1262,
      "step": 2410
    },
    {
      "epoch": 0.9718875502008032,
      "grad_norm": 0.12952326238155365,
      "learning_rate": 0.00013531459170013386,
      "loss": 0.157,
      "step": 2420
    },
    {
      "epoch": 0.9759036144578314,
      "grad_norm": 0.18679475784301758,
      "learning_rate": 0.00013504685408299865,
      "loss": 0.1961,
      "step": 2430
    },
    {
      "epoch": 0.9799196787148594,
      "grad_norm": 0.1517753005027771,
      "learning_rate": 0.00013477911646586345,
      "loss": 0.1532,
      "step": 2440
    },
    {
      "epoch": 0.9839357429718876,
      "grad_norm": 0.1255030333995819,
      "learning_rate": 0.00013451137884872824,
      "loss": 0.1724,
      "step": 2450
    },
    {
      "epoch": 0.9879518072289156,
      "grad_norm": 0.18230731785297394,
      "learning_rate": 0.00013424364123159306,
      "loss": 0.1234,
      "step": 2460
    },
    {
      "epoch": 0.9919678714859438,
      "grad_norm": 0.16073386371135712,
      "learning_rate": 0.00013397590361445785,
      "loss": 0.1214,
      "step": 2470
    },
    {
      "epoch": 0.9959839357429718,
      "grad_norm": 0.14152446389198303,
      "learning_rate": 0.00013370816599732262,
      "loss": 0.1497,
      "step": 2480
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.2896365821361542,
      "learning_rate": 0.0001334404283801874,
      "loss": 0.1469,
      "step": 2490
    }
  ],
  "logging_steps": 10,
  "max_steps": 7470,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.702044930919629e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
